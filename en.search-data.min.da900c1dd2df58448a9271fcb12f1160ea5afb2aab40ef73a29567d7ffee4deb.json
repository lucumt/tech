[{"id":0,"href":"/docs/db/theory/8-key-data-structures-that-power-modern-databases/","title":"8ä¸ªå…³é”®æ•°æ®ç»“æ„","section":"ç†è®º","content":"\n8 Key Data Structures That Power Modern Databases\nSkiplist: a common in-memory index type. Used in Redis Hash index: a very common implementation of the â€œMapâ€ data structure (or â€œCollectionâ€) SSTable: immutable on-disk â€œMapâ€ implementation LSM tree: Skiplist + SSTable. High write throughput B-tree: disk-based solution. Consistent read/write performance Inverted index: used for document indexing. Used in Lucene Suffix tree: for string pattern search R-tree: multi-dimension search, such as finding the nearest neighbor "},{"id":1,"href":"/docs/db/es/top-6-es-use-cases/","title":"ESä½¿ç”¨æœ€é«˜çš„6ä¸ªåœºæ™¯","section":"ElasticSearch","content":"\nTop 6 ElasticSearch Use Cases. . . Elasticsearch is widely used for its powerful and versatile search capabilities. The diagram below shows the top 6 use cases:\nğŸ”¹ Full-Text Search Elasticsearch excels in full-text search scenarios due to its robust, scalable, and fast search capabilities. It allows users to perform complex queries with near real-time responses.\nğŸ”¹ Real-Time Analytics Elasticsearch\u0026rsquo;s ability to perform analytics in real-time makes it suitable for dashboards that track live data, such as user activity, transactions, or sensor outputs.\nğŸ”¹ Machine Learning With the addition of the machine learning feature in X-Pack, Elasticsearch can automatically detect anomalies, patterns, and trends in the data.\nğŸ”¹ Geo-Data Applications Elasticsearch supports geo-data through geospatial indexing and searching capabilities. This is useful for applications that need to manage and visualize geographical information, such as mapping and location-based services.\nğŸ”¹ Log and Event Data Analysis Organizations use Elasticsearch to aggregate, monitor, and analyze logs and event data from various sources. It\u0026rsquo;s a key component of the ELK stack (Elasticsearch, Logstash, Kibana), which is popular for managing system and application logs to identify issues and monitor system health.\nğŸ”¹ Security Information and Event Management (SIEM) Elasticsearch can be used as a tool for SIEM, helping organizations to analyze security events in real time.\n"},{"id":2,"href":"/docs/tools/git/git-commands/","title":"Gitå¸¸ç”¨æŒ‡ä»¤","section":"Git","content":" æ·»åŠ ä¸å–æ¶ˆä»£ç†(å±€éƒ¨)\n1# æ·»åŠ ä»£ç† å…¶ä¸­7890æ˜¯clashå¯¹å¤–æš´éœ²çš„ç«¯å£ 2git config http.proxy http://127.0.0.1:7890 3git config https.proxy https://127.0.0.1:7890 4 5# å»æ‰ä»£ç† 6git config --unset http.proxy 7git config --unset https.proxy æ·»åŠ ä¸å–æ¶ˆä»£ç†(å…¨å±€)\n1# æ·»åŠ ä»£ç† 2git config --global http.proxy http://127.0.0.1:7890 3git config --global https.proxy https://127.0.0.1:7890 4 5# å»æ‰ä»£ç† 6git config --global --unset http.proxy 7git config --global --unset https.proxy gitæ·»åŠ åˆ«å\nåœ¨Windowsç³»ç»Ÿä¸­éœ€è¦åœ¨$PWD/etc/bash.bashrcçš„å°¾éƒ¨æ·»åŠ ç±»ä¼¼å¦‚ä¸‹å†…å®¹\n1alias git-log=\u0026#34;git log --abbrev-commit --pretty=oneline --graph\u0026#34; 2alias git-logs=\u0026#34;git log --pretty=format:%h%x09%an%x09%ad%x09%s --graph --date=format:\u0026#39;%Y-%m-%d %H:%M:%S\u0026#39;\u0026#34; "},{"id":3,"href":"/docs/code/front/how-does-javascript-work/","title":"JavaScriptæ˜¯å¦‚ä½•æ‰§è¡Œçš„","section":"å‰ç«¯","content":"\nThe cheat sheet below shows most important characteristics of Javascript.\nğŸ”¹ Interpreted Language JavaScript code is executed by the browser or JavaScript engine rather than being compiled into machine language beforehand. This makes it highly portable across different platforms. Modern engines such as V8 utilize Just-In-Time (JIT) technology to compile code into directly executable machine code.\nğŸ”¹ Function is First-Class Citizen In JavaScript, functions are treated as first-class citizens, meaning they can be stored in variables, passed as arguments to other functions, and returned from functions.\nğŸ”¹ Dynamic Typing JavaScript is a loosely typed or dynamic language, meaning we don\u0026rsquo;t have to declare a variable\u0026rsquo;s type ahead of time, and the type can change at runtime.\nğŸ”¹ Client-Side Execution JavaScript supports asynchronous programming, allowing operations like reading files, making HTTP requests, or querying databases to run in the background and trigger callbacks or promises when complete. This is particularly useful in web development for improving performance and user experience.\nğŸ”¹ Prototype-Based OOP Unlike class-based object-oriented languages, JavaScript uses prototypes for inheritance. This means that objects can inherit properties and methods from other objects.\nğŸ”¹ Automatic Garbage Collection Garbage collection in JavaScript is a form of automatic memory management. The primary goal of garbage collection is to reclaim memory occupied by objects that are no longer in use by the program, which helps prevent memory leaks and optimizes the performance of the application.\nğŸ”¹ Compared with Other Languages JavaScript is special compared to programming languages like Python or Java because of its position as a major language for web development.\nWhile Python is known to provide good code readability and versatility, and Java is known for its structure and robustness, JavaScript is an interpreted language that runs directly on the browser without compilation, emphasizing flexibility and dynamism.\nğŸ”¹ Relationship with Typescript TypeScript is a superset of JavaScript, which means that it extends JavaScript by adding features to the language, most notably type annotations. This relationship allows any valid JavaScript code to also be considered valid TypeScript code.\nğŸ”¹ Popular Javascript Frameworks React is known for its flexibility and large number of community-driven plugins, while Vue is clean and intuitive with highly integrated and responsive features. Angular, on the other hand, offers a strict set of development specifications for enterprise-level JS development.\n"},{"id":4,"href":"/docs/code/mq/top-5-kafka-use-cases/","title":"Kafkaçš„5ç§ä½¿ç”¨åœºæ™¯","section":"æ¶ˆæ¯ä¸­é—´ä»¶","content":"\n"},{"id":5,"href":"/docs/code/block/mysql-batch-create-data/","title":"MySQLæ‰¹é‡åˆ¶é€ æ•°æ®","section":"ä»£ç å—","content":"åˆ©ç”¨mysqlçš„å­˜å‚¨è¿‡ç¨‹å¿«é€Ÿæ’å…¥å¤§é‡æ•°æ®\n1DELIMITER $$ 2 3USE `test`$$ 4 5DROP PROCEDURE IF EXISTS `add_user_batch`$$ 6 7CREATE DEFINER=`root`@`%` PROCEDURE `add_user_batch`(IN COUNT INT) 8BEGIN 9 DECLARE i INT; 10 DECLARE t_name VARCHAR(8); 11 DECLARE t_tag VARCHAR(20); 12 DECLARE t_age INT(2); 13 DECLARE t_sql_template VARCHAR(100); 14 DECLARE t_sql TEXT; 15 DECLARE t_tag_mod_val INT DEFAULT(25); 16 DECLARE t_commit_mod_val INT DEFAULT(100); 17 18 DECLARE t_start_time DATETIME; 19 DECLARE t_end_time DATETIME; 20 21 TRUNCATE TABLE `system_user`; 22 23 SET t_start_time=NOW(); 24 SET t_sql_template = \u0026#34;INSERT INTO `system_user`(NAME, age, tag) VALUES\u0026#34;; 25 SET t_sql = t_sql_template; 26 SET i = 1; 27 WHILE i \u0026lt;= COUNT 28 DO 29 SET t_age = FLOOR(1 + RAND() * 60); 30 SET t_name = LEFT(UUID(), 8); 31 -- ç»™tagéšæœºåˆ¶é€ ç©ºå€¼ 32 IF MOD(i, t_tag_mod_val) = 0 THEN 33 SET t_tag = \u0026#34;NULL\u0026#34;; 34 ELSE 35 SET t_tag = CONCAT(\u0026#34;\u0026#39;\u0026#34;,LEFT(UUID(), 8),\u0026#34;\u0026#39;\u0026#34;); 36 END IF; 37 38 SET t_sql = CONCAT(t_sql,\u0026#34;(\u0026#39;\u0026#34;,t_name,\u0026#34;\u0026#39;,\u0026#34;,t_age,\u0026#34;,\u0026#34;,t_tag,\u0026#34;)\u0026#34;); 39 40 IF MOD(i,t_commit_mod_val) != 0 THEN 41 SET t_sql = CONCAT(t_sql,\u0026#34;,\u0026#34;); 42 ELSE 43 SET t_sql = CONCAT(t_sql,\u0026#34;;\u0026#34;); 44 -- åªè¦è¾¾åˆ°t_commit_mod_valè¦æ±‚çš„æ¬¡æ•°ï¼Œå°±æ‰§è¡Œå¹¶æäº¤ 45 SET @insert_sql = t_sql; 46 PREPARE stmt FROM @insert_sql; 47 EXECUTE stmt; 48 DEALLOCATE PREPARE stmt; 49 COMMIT; 50 SET t_sql=t_sql_template; 51 END IF; 52 SET i = i + 1; 53 END WHILE; 54 55 -- ä¸èƒ½è¢«t_commit_mod_valæ•´é™¤æ—¶ï¼Œä½™ä¸‹çš„æ•°æ®å¤„ç† 56 IF LENGTH(t_sql) \u0026gt; LENGTH(t_sql_template) THEN 57 SET t_sql=CONCAT(SUBSTRING(t_sql,1,LENGTH(t_sql)-1),\u0026#39;;\u0026#39;); 58 SET @insert_sql = t_sql; 59 PREPARE stmt FROM @insert_sql; 60 EXECUTE stmt; 61 DEALLOCATE PREPARE stmt; 62 COMMIT; 63 END IF; 64 SET t_end_time=NOW(); 65 SELECT CONCAT(\u0026#39;insert data success,time cost \u0026#39;,TIMEDIFF(t_end_time,t_start_time)) AS finishedTag; 66END$$ 67 68DELIMITER ; "},{"id":6,"href":"/docs/db/redis/redis-architecture-evolve/","title":"Redisæ¶æ„æ¼”è¿›","section":"Redis","content":"\nRome wasn\u0026rsquo;t built in a day.\nHow does Redis architecture evolve?\nRedis is a popular in-memory cache. How did it evolve to the architecture it is today?\nğŸ”¹ 2010 - Standalone Redis When Redis 1.0 was released in 2010, the architecture was quite simple. It is usually used as a cache to the business application.\nHowever, Redis stores data in memory. When we restart Redis, we will lose all the data and the traffic directly hits the database.\nğŸ”¹ 2013 - Persistence When Redis 2.8 was released in 2013, it addressed the previous restrictions. Redis introduced RDB in-memory snapshots to persist data. It also supports AOF (Append-Only-File), where each write command is written to an AOF file.\nğŸ”¹ 2013 - Replication Redis 2.8 also added replication to increase availability. The primary instance handles real-time read and write requests, while replica synchronizes the primary\u0026rsquo;s data.\nğŸ”¹ 2013 - Sentinel Redis 2.8 introduced Sentinel to monitor the Redis instances in real time. is a system designed to help managing Redis instances. It performs the following four tasks: monitoring, notification, automatic failover and configuration provider.\nğŸ”¹ 2015 - Cluster In 2015, Redis 3.0 was released. It added Redis clusters.\nA Redis cluster is a distributed database solution that manages data through sharding. The data is divided into 16384 slots, and each node is responsible for a portion of the slot.\nğŸ”¹ Looking Ahead Redis is popular because of its high performance and rich data structures that dramatically reduce the complexity of developing a business application.\nIn 2017, Redis 5.0 was released, adding the stream data type.\nIn 2020, Redis 6.0 was released, introducing the multi-threaded I/O in the network module. Redis model is divided into the network module and the main processing module. The Redis developers the network module tends to become a bottleneck in the system.\n"},{"id":7,"href":"/docs/other/stop-the-war/","title":"Stop The War","section":"å…¶å®ƒ","content":"\n"},{"id":8,"href":"/docs/network/tcp-status-change/","title":"TCPçŠ¶æ€å˜åŒ–å›¾","section":"ç½‘ç»œ","content":"\n"},{"id":9,"href":"/docs/security/identity-manage-types/","title":"ä¸åŒçš„èº«ä»½ç®¡ç†æ–¹å¼","section":"å®‰å…¨","content":"\nWhen you login to a website, your identity needs to be managed. Here is how different solutions work:\n- Session - The server stores your identity and gives the browser a session ID cookie. This allows the server to track login state. But cookies don\u0026rsquo;t work well across devices.\n- Token - Your identity is encoded into a token sent to the browser. The browser sends this token on future requests for authentication. No server session storage is required. But tokens need encryption/decryption.\n- JWT - JSON Web Tokens standardize identity tokens using digital signatures for trust. The signature is contained in the token so no server session is needed.\n- SSO - Single Sign On uses a central authentication service. This allows a single login to work across multiple sites.\n- OAuth2 - Allows limited access to your data on one site by another site, without giving away passwords.\n- QR Code - Encodes a random token into a QR code for mobile login. Scanning the code logs you in without typing a password.\n"},{"id":10,"href":"/docs/security/authentication-types-compare/","title":"ä¸åŒè®¤è¯æ–¹å¼çš„å¯¹æ¯”","section":"å®‰å…¨","content":"\n"},{"id":11,"href":"/docs/code/theory/language-compare/","title":"ä¸åŒè¯­è¨€çš„å·¥ä½œåŸç†","section":"ç†è®º","content":"\nHow Do C++, Java, Python Work?\nThe diagram shows how the compilation and execution work.\nCompiled languages are compiled into machine code by the compiler. The machine code can later be executed directly by the CPU. Examples: C, C++, Go.\nA bytecode language like Java, compiles the source code into bytecode first, then the JVM executes the program. Sometimes JIT (Just-In-Time) compiler compiles the source code into machine code to speed up the execution. Examples: Java, C#\nInterpreted languages are not compiled. They are interpreted by the interpreter during runtime. Examples: Python, Javascript, Ruby\nCompiled languages in general run faster than interpreted languages.\n"},{"id":12,"href":"/docs/cloud/cloud-tech-stack/","title":"äº‘åŸç”ŸæŠ€æœ¯æ ˆ","section":"äº‘åŸç”Ÿ","content":"\n"},{"id":13,"href":"/docs/code/framework/grpc/what-is-grpc/","title":"ä»€ä¹ˆæ˜¯gRPC","section":"gRPC","content":"\nWhat is gRPC?\nThe diagram below shows important aspects of understanding gRPC.\ngRPC is a high-performance, open-source universal RPC (Remote Procedure Call) framework initially developed by Google. It leverages HTTP/2 for transport, Protocol Buffers as the interface description language, and provides features such as authentication, load balancing, and more. gRPC is designed to enable efficient and robust communication between services in a microservices architecture, making it a popular choice for building distributed systems and APIs.\nKey Features of gRPC:\nProtocol Buffers: By default, gRPC uses Protocol Buffers (proto files) as its interface definition language (IDL). This makes gRPC messages smaller and faster compared to JSON or XML. HTTP/2 Based Transport: gRPC uses HTTP/2 for transport, which allows for many improvements over HTTP/1.x. Multiple Language Support: gRPC supports a wide range of programming languages. Bi-Directional Streaming: gRPC supports streaming requests and responses, allowing for the development of sophisticated real-time applications with bidirectional communication like chat services. "},{"id":14,"href":"/docs/algorithm/sorted-stack/","title":"å•è°ƒæ ˆ","section":"ç®—æ³•","content":" å•è°ƒé€’å¢ï¼Œä»æ ˆé¡¶åˆ°æ ˆåº•ä¾æ¬¡é€’å¢\n1public static void sortStackIncrease() { 2 int[] data = {2, 7, 5, 4, 6, 3, 4, 2}; 3 Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); 4 for (Integer val : data) { 5 while (!stack.isEmpty() \u0026amp;\u0026amp; stack.peek() \u0026lt;= val) { 6 stack.pop(); 7 } 8 stack.push(val); 9 } 10 System.out.println(stack); 11} å•è°ƒé€’å‡ï¼Œä»æ ˆé¡¶åˆ°æ ˆåº•ä¾æ¬¡é€’å‡\n1public static void sortStackDecrease() { 2 int[] data = {4, 3, 2, 5, 7, 4, 6, 8}; 3 Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); 4 for (Integer val : data) { 5 while (!stack.isEmpty() \u0026amp;\u0026amp; stack.peek() \u0026gt;= val) { 6 stack.pop(); 7 } 8 stack.push(val); 9 } 10 System.out.println(stack); 11} æ‰¾åˆ°æ•°ç»„å³è¾¹ç¬¬ä¸€ä¸ªæ¯”å®ƒå¤§çš„å…ƒç´ ï¼Œç¬¬1ç§å®ç°ï¼ŒåŸºäºå€’åºå·ï¼Œè‡ªå·±æœ‰äº›çœ‹ä¸æ‡‚\n1public static void nextGreaterElement() { 2 int[] data = {2, 7, 5, 4, 6, 3, 4, 2}; 3 int[] result = new int[data.length]; 4 Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); 5 // å•è°ƒé€’å¢ 6 for (int i = data.length - 1; i \u0026gt;= 0; i--) { 7 int val = data[i]; 8 while (!stack.isEmpty() \u0026amp;\u0026amp; stack.peek() \u0026lt;= val) { 9 stack.pop(); 10 } 11 result[i] = stack.empty() ? -1 : stack.peek(); 12 stack.push(val); 13 } 14 System.out.println(stack); 15 System.out.println(StringUtils.join(ArrayUtils.toObject(result), \u0026#34;,\u0026#34;)); 16} ç¬¦åˆå¸¸è§„æ€ç»´çš„ä»å·¦åˆ°å³çš„å®ç°ï¼Œæ­¤æ—¶è®°å½•çš„æ˜¯ä¸‹æ ‡å€¼\n1public static void nextGreaterElement() { 2 int[] data = {2, 7, 5, 4, 6, 3, 4, 2}; 3 int[] result = new int[data.length]; 4 Arrays.fill(result, -1); 5 Stack\u0026lt;Integer\u0026gt; stack = new Stack\u0026lt;\u0026gt;(); 6 // å•è°ƒé€’å¢ 7 for (int i = 0; i \u0026lt; data.length; i++) { 8 int val = data[i]; 9 while (!stack.isEmpty() \u0026amp;\u0026amp; data[stack.peek()] \u0026lt; val) { 10 result[stack.pop()] = val; 11 } 12 stack.push(i); 13 } 14 System.out.println(stack); 15 System.out.println(StringUtils.join(ArrayUtils.toObject(result), \u0026#34;,\u0026#34;)); 16} "},{"id":15,"href":"/docs/bigdata/data-pipelines-overview/","title":"æ•°æ®ç®¡é“æ¦‚è§ˆ","section":"å¤§æ•°æ®","content":"\nData pipelines are a fundamental component of managing and processing data efficiently within modern systems. These pipelines typically encompass 5 predominant phases: Collect, Ingest, Store, Compute, and Consume.\n\\1. Collect: Data is acquired from data stores, data streams, and applications, sourced remotely from devices, applications, or business systems.\n\\2. Ingest: During the ingestion process, data is loaded into systems and organized within event queues.\n\\3. Store: Post ingestion, organized data is stored in data warehouses, data lakes, and data lakehouses, along with various systems like databases, ensuring post-ingestion storage.\n\\4. Compute: Data undergoes aggregation, cleansing, and manipulation to conform to company standards, including tasks such as format conversion, data compression, and partitioning. This phase employs both batch and stream processing techniques.\n\\5. Consume: Processed data is made available for consumption through analytics and visualization tools, operational data stores, decision engines, user-facing applications, dashboards, data science, machine learning services, business intelligence, and self-service analytics.\nThe efficiency and effectiveness of each phase contribute to the overall success of data-driven operations within an organization.\n"},{"id":16,"href":"/docs/linux/system/linux-file-system/","title":"æ–‡ä»¶ç±»å‹è¯´æ˜","section":"Linuxç³»ç»Ÿ","content":"\nThe Linux file system used to resemble an unorganized town where individuals constructed their houses wherever they pleased. However, in 1994, the Filesystem Hierarchy Standard (FHS) was introduced to bring order to the Linux file system.\nBy implementing a standard like the FHS, software can ensure a consistent layout across various Linux distributions. Nonetheless, not all Linux distributions strictly adhere to this standard. They often incorporate their own unique elements or cater to specific requirements.\nTo become proficient in this standard, you can begin by exploring. Utilize commands such as \u0026ldquo;cd\u0026rdquo; for navigation and \u0026ldquo;ls\u0026rdquo; for listing directory contents. Imagine the file system as a tree, starting from the root (/). With time, it will become second nature to you, transforming you into a skilled Linux administrator.\n"},{"id":17,"href":"/docs/ai/what-is-an-ai-agent/","title":"æ™ºèƒ½ä½“è§£é‡Š","section":"AI","content":"What is an AI Agent?\nAn AI agent is a software program that can interact with its environment, gather data, and use that data to achieve predetermined goals. AI agents can choose the best actions to perform to meet those goals.\nKey characteristics of AI agents are as follows:\nAn agent can perform autonomous actions without constant human intervention. Also, they can have a human in the loop to maintain control. - Agents have a memory to store individual preferences and allow for personalization. It can also store knowledge. An LLM can undertake information processing and decision-making functions. - Agents must be able to perceive and process the information available from their environment. - Agents can also use tools such as accessing the internet, using code interpreters and making API calls. - Agents can also collaborate with other agents or humans.\nMultiple types of AI agents are available such as learning agents, simple reflex agents, model-based reflex agents, goal-based agents, and utility-based agents.\nA system with AI agents can be built with different architectural approaches.\nSingle Agent: Agents can serve as personal assistants. Multi-Agent: Agents can interact with each other in collaborative or competitive ways. Human Machine: Agents can interact with humans to execute tasks more efficiently. "},{"id":18,"href":"/docs/test/test-method-compare/","title":"æµ‹è¯•æ–¹æ³•å¯¹æ¯”","section":"æµ‹è¯•","content":"\n"},{"id":19,"href":"/docs/monitor/fantastic-four-of-system-design/","title":"ç³»ç»Ÿè®¾è®¡çš„4å¤§å…³é”®","section":"è¿ç»´","content":"\nWho are the Fantastic Four of System Design?\nScalability, Availability, Reliability, and Performance.\nThey are the most critical components to crafting successful software systems.\nLetâ€™s look at each of them with implementation techniques:\n1 - Scalability Scalability ensures that your application can handle more load without compromising performance.\n2 - Availability Availability makes sure that your application is always ready to serve the users and downtime is minimal.\n3 - Reliability Reliability is about building software that consistently delivers correct results.\n4 - Performance Performance is the ability of a system to carry out its tasks at an expected rate under peak load using available resources.\n"},{"id":20,"href":"/docs/code/language/java/thread-lifecycle/","title":"çº¿ç¨‹ç”Ÿå‘½å‘¨æœŸè¯´æ˜","section":"java","content":" è¯´æ˜\nçº¿ç¨‹ç”Ÿå‘½å‘¨æœŸå¾ˆè€ƒéªŒJavaå¤šçº¿ç¨‹ç¼–ç¨‹çš„åŸºæœ¬åŠŸ å›¾è§£1 # å›¾è§£2 # "},{"id":21,"href":"/docs/microservice/develop/what-does-api-gateway-do/","title":"ç½‘å…³çš„ä½œç”¨","section":"å¾®æœåŠ¡å¼€å‘","content":"\nWhat does API gateway do? The diagram below shows the detail.\nStep 1 - The client sends an HTTP request to the API gateway.\nStep 2 - The API gateway parses and validates the attributes in the HTTP request.\nStep 3 - The API gateway performs allow-list/deny-list checks.\nStep 4 - The API gateway talks to an identity provider for authentication and authorization.\nStep 5 - The rate limiting rules are applied to the request. If it is over the limit, the request is rejected.\nSteps 6 and 7 - Now that the request has passed basic checks, the API gateway finds the relevant service to route to by path matching.\nStep 8 - The API gateway transforms the request into the appropriate protocol and sends it to backend microservices.\nSteps 9-12: The API gateway can handle errors properly, and deals with faults if the error takes a longer time to recover (circuit break). It can also leverage ELK (Elastic-Logstash-Kibana) stack for logging and monitoring. We sometimes cache data in the API gateway.\nOver to you: 1) Whatâ€™s the difference between a load balancer and an API gateway? 2) Do we need to use different API gateways for PC, mobile and browser separately?\n"},{"id":22,"href":"/docs/tools/network-config/","title":"ç½‘ç»œé…ç½®","section":"å·¥å…·","content":" Windowsä¸­æ·»åŠ Clashä»£ç†\n1set http_proxy=http://127.0.0.1:7890 \u0026amp; set https_proxy=http://127.0.0.1:7890 "},{"id":23,"href":"/docs/cloud/top-6-clould-message-patterns/","title":"6ç§äº‘æ¶ˆæ¯ä¼ é€’æ¨¡å¼","section":"äº‘åŸç”Ÿ","content":"\nHow do services communicate with each other? The diagram below shows 6 cloud messaging patterns.\nğŸ”¹ Asynchronous Request-Reply\nThis pattern aims at providing determinism for long-running backend tasks. It decouples backend processing from frontend clients.\nIn the diagram below, the client makes a synchronous call to the API, triggering a long-running operation on the backend. The API returns an HTTP 202 (Accepted) status code, acknowledging that the request has been received for processing.\nğŸ”¹ Publisher-Subscriber\nThis pattern targets decoupling senders from consumers, and avoiding blocking the sender to wait for a response.\nğŸ”¹ Claim Check\nThis pattern solves the transmision of large messages. It stores the whole message payload into a database and transmits only the reference to the message, which will be used later to retrieve the payload from the database.\nğŸ”¹ Priority Queue\nThis pattern prioritizes requests sent to services so that requests with a higher priority are received and processed more quickly than those with a lower priority.\nğŸ”¹ Saga\nSaga is used to manage data consistency across multiple services in distributed systems, especially in microservices architectures where each service manages its own database.\nThe saga pattern addresses the challenge of maintaining data consistency without relying on distributed transactions, which are difficult to scale and can negatively impact system performance.\nğŸ”¹ Competing Consumers\nThis pattern enables multiple concurrent consumers to process messages received on the same messaging channel. There is no need to configure complex coordination between the consumers. However, this pattern cannot guarantee message ordering.\n"},{"id":24,"href":"/docs/test/9-types-api-testing/","title":"9ä¸­APIæµ‹è¯•æ–¹å¼","section":"æµ‹è¯•","content":"\nExplaining 9 types of API testing.\nğŸ”¹ Smoke Testing This is done after API development is complete. Simply validate if the APIs are working and nothing breaks.\nğŸ”¹ Functional Testing This creates a test plan based on the functional requirements and compares the results with the expected results.\nğŸ”¹ Integration Testing This test combines several API calls to perform end-to-end tests. The intra-service communications and data transmissions are tested.\nğŸ”¹ Regression Testing This test ensures that bug fixes or new features shouldnâ€™t break the existing behaviors of APIs.\nğŸ”¹ Load Testing This tests applicationsâ€™ performance by simulating different loads. Then we can calculate the capacity of the application.\nğŸ”¹ Stress Testing We deliberately create high loads to the APIs and test if the APIs are able to function normally.\nğŸ”¹ Security Testing This tests the APIs against all possible external threats.\nğŸ”¹ UI Testing This tests the UI interactions with the APIs to make sure the data can be displayed properly.\nğŸ”¹ Fuzz Testing This injects invalid or unexpected input data into the API and tries to crash the API. In this way, it identifies the API vulnerabilities.\n"},{"id":25,"href":"/docs/ai/how-does-chatgpt-work/","title":"ChatGPTå·¥ä½œåŸç†","section":"AI","content":"\nHow does ChatGPT-like system work?\nSince OpenAI hasn\u0026rsquo;t provided all the details, some parts of the diagram may be inaccurate.\nWe attempted to explain how it works in the diagram below. The process can be broken down into two parts.\nTraining. To train a ChatGPT model, there are two stages:\n- Pre-training: In this stage, we train a GPT model (decoder-only transformer) on a large chunk of internet data. The objective is to train a model that can predict future words given a sentence in a way that is grammatically correct and semantically meaningful similar to the internet data. After the pre-training stage, the model can complete given sentences, but it is not capable of responding to questions.\n- Fine-tuning: This stage is a 3-step process that turns the pre-trained model into a question-answering ChatGPT model:\n1). Collect training data (questions and answers), and fine-tune the pre-trained model on this data. The model takes a question as input and learns to generate an answer similar to the training data.\n2). Collect more data (question, several answers) and train a reward model to rank these answers from most relevant to least relevant.\n3). Use reinforcement learning (PPO optimization) to fine-tune the model so the model\u0026rsquo;s answers are more accurate.\nAnswer a prompt\nğŸ”¹Step 1: The user enters the full question, â€œExplain how a classification algorithm worksâ€.\nğŸ”¹Step 2: The question is sent to a content moderation component. This component ensures that the question does not violate safety guidelines and filters inappropriate questions.\nğŸ”¹Steps 3-4: If the input passes content moderation, it is sent to the chatGPT model. If the input doesnâ€™t pass content moderation, it goes straight to template response generation.\nğŸ”¹Step 5-6: Once the model generates the response, it is sent to a content moderation component again. This ensures the generated response is safe, harmless, unbiased, etc.\nğŸ”¹Step 7: If the input passes content moderation, it is shown to the user. If the input doesnâ€™t pass content moderation, it goes to template response generation and shows a template answer to the user.\n"},{"id":26,"href":"/docs/monitor/top-9-cases-behind-cpu-high-usage/","title":"CPUä½¿ç”¨è¾¾åˆ°100%çš„9ä¸ªåœºæ™¯","section":"è¿ç»´","content":"\n"},{"id":27,"href":"/docs/code/framework/grpc/how-does-grpc-work/","title":"gRPCå¦‚ä½•å·¥ä½œçš„","section":"gRPC","content":"\nHow does ğ ğ‘ğğ‚ work? RPC (Remote Procedure Call) is called â€œğ«ğğ¦ğ¨ğ­ğâ€ because it enables communications between remote services when services are deployed to different servers under microservice architecture. From the userâ€™s point of view, it acts like a local function call.\nThe diagram below illustrates the overall data flow for ğ ğ‘ğğ‚.\nStep 1: A REST call is made from the client. The request body is usually in JSON format.\nSteps 2 - 4: The order service (gRPC client) receives the REST call, transforms it, and makes an RPC call to the payment service. gPRC encodes the ğœğ¥ğ¢ğğ§ğ­ ğ¬ğ­ğ®ğ› into a binary format and sends it to the low-level transport layer.\nStep 5: gRPC sends the packets over the network via HTTP2. Because of binary encoding and network optimizations, gRPC is said to be 5X faster than JSON.\nSteps 6 - 8: The payment service (gRPC server) receives the packets from the network, decodes them, and invokes the server application.\nSteps 9 - 11: The result is returned from the server application, and gets encoded and sent to the transport layer.\nSteps 12 - 14: The order service receives the packets, decodes them, and sends the result to the client application.\n"},{"id":28,"href":"/docs/network/http-status/","title":"HTTPçŠ¶æ€ç è¯´æ˜","section":"ç½‘ç»œ","content":"\n"},{"id":29,"href":"/docs/code/mq/can-kafka-lose-messages/","title":"kakfaä¸¢å¤±æ¶ˆæ¯çš„åœºæ™¯","section":"æ¶ˆæ¯ä¸­é—´ä»¶","content":"\nError handling is one of the most important aspects of building reliable systems.\nToday, we will discuss an important topic: Can Kafka lose messages?\nA common belief among many developers is that Kafka, by its very design, guarantees no message loss. However, understanding the nuances of Kafka\u0026rsquo;s architecture and configuration is essential to truly grasp how and when it might lose messages, and more importantly, how to prevent such scenarios.\nThe diagram below shows how a message can be lost during its lifecycle in Kafka.\nğŸ”¹ Producer When we call producer.send() to send a message, it doesn\u0026rsquo;t get sent to the broker directly. There are two threads and a queue involved in the message-sending process:\n\\1. Application thread \\2. Record accumulator \\3. Sender thread (I/O thread)\nWe need to configure proper â€˜acksâ€™ and â€˜retriesâ€™ for the producer to make sure messages are sent to the broker.\nğŸ”¹ Broker A broker cluster should not lose messages when it is functioning normally. However, we need to understand which extreme situations might lead to message loss:\n\\1. The messages are usually flushed to the disk asynchronously for higher I/O throughput, so if the instance is down before the flush happens, the messages are lost.\n\\2. The replicas in the Kafka cluster need to be properly configured to hold a valid copy of the data. The determinism in data synchronization is important.\nğŸ”¹ Consumer Kafka offers different ways to commit messages. Auto-committing might acknowledge the processing of records before they are actually processed. When the consumer is down in the middle of processing, some records may never be processed.\nA good practice is to combine both synchronous and asynchronous commits, where we use asynchronous commits in the processing loop for higher throughput and synchronous commits in exception handling to make sure the the last offset is always committed.\n"},{"id":30,"href":"/docs/linux/","title":"Linux","section":"Docs","content":" Linux Shellå¿«æ·é”® # å…³äºå¿«æ·é”®çš„æ–‡ç« å‚è€ƒ\nVimå¿«æ·é”®[åšå®¢å›­] Vimå¿«æ·é”®[Linux.cn] å¤åˆ¶ç²˜è´´ # å¤åˆ¶\nå•è¡Œå¤åˆ¶ï¼Œå°†å…‰æ ‡ç§»åŠ¨åˆ°å°†è¦å¤åˆ¶çš„è¡Œå¤„ï¼ŒæŒ‰yyè¿›è¡Œå¤åˆ¶ å¤šè¡Œå¤åˆ¶ï¼Œå°†å…‰æ ‡ç§»åŠ¨åˆ°å°†è¦å¤åˆ¶çš„é¦–è¡Œå¤„ï¼ŒæŒ‰nyyå¤åˆ¶nè¡Œï¼Œå…¶ä¸­nä¸º1ã€2ã€3â€¦â€¦ yyå¤åˆ¶ä¸€è¡Œï¼Œnyyå¤åˆ¶nè¡Œ\nç²˜è´´ï¼Œå°†å…‰æ ‡ç§»åŠ¨åˆ°å°†è¦ç²˜è´´çš„è¡Œå¤„ï¼ŒæŒ‰pè¿›è¡Œç²˜è´´\næ–¹å‘é”® # å¿«æ·é”® åŠŸèƒ½ h å…‰æ ‡å‘å·¦ç§»åŠ¨ä¸€ä¸ªå­—ç¬¦ j æˆ– Ctrl + J å…‰æ ‡å‘ä¸‹ç§»åŠ¨ä¸€è¡Œ k æˆ– Ctrl + P å…‰æ ‡å‘ä¸Šç§»åŠ¨ä¸€è¡Œ l å…‰æ ‡å‘å³ç§»åŠ¨ä¸€ä¸ªå­—ç¬¦ 0 ï¼ˆæ•°å­— 0ï¼‰ç§»åŠ¨å…‰æ ‡è‡³æœ¬è¡Œå¼€å¤´ $ ç§»åŠ¨å…‰æ ‡è‡³æœ¬è¡Œæœ«å°¾ ^ ç§»åŠ¨å…‰æ ‡è‡³æœ¬è¡Œç¬¬ä¸€ä¸ªéç©ºå­—ç¬¦å¤„ w å‘å‰ç§»åŠ¨ä¸€ä¸ªè¯ ï¼ˆä¸Šä¸€ä¸ªå­—æ¯å’Œæ•°å­—ç»„æˆçš„è¯ä¹‹åï¼‰ W å‘å‰ç§»åŠ¨ä¸€ä¸ªè¯ ï¼ˆä»¥ç©ºæ ¼åˆ†éš”çš„è¯ï¼‰ 5w å‘å‰ç§»åŠ¨äº”ä¸ªè¯ b å‘åç§»åŠ¨ä¸€ä¸ªè¯ ï¼ˆä¸‹ä¸€ä¸ªå­—æ¯å’Œæ•°å­—ç»„æˆçš„è¯ä¹‹å‰ï¼‰ B å‘åç§»åŠ¨ä¸€ä¸ªè¯ ï¼ˆä»¥ç©ºæ ¼åˆ†éš”çš„è¯ï¼‰ 5b å‘åç§»åŠ¨äº”ä¸ªè¯ G ç§»åŠ¨è‡³æ–‡ä»¶æœ«å°¾ gg ç§»åŠ¨è‡³æ–‡ä»¶å¼€å¤´ åˆ é™¤ # å¿«æ·é”® åŠŸèƒ½ x åˆ é™¤å…‰æ ‡å¤„å­—ç¬¦ dw åˆ é™¤ä¸€ä¸ªè¯ d0 åˆ è‡³è¡Œé¦– d$ åˆ è‡³è¡Œæœ« d) åˆ è‡³å¥æœ« dgg åˆ è‡³æ–‡ä»¶å¼€å¤´ dG åˆ è‡³æ–‡ä»¶æœ«å°¾ dd åˆ é™¤è¯¥è¡Œ 3dd åˆ é™¤ä¸‰è¡Œ æœç´¢æ›¿æ¢ # å¿«æ·é”® åŠŸèƒ½ /search_text æ£€ç´¢æ–‡æ¡£ï¼Œåœ¨æ–‡æ¡£åé¢çš„éƒ¨åˆ†æœç´¢ search_text ?search_text æ£€ç´¢æ–‡æ¡£ï¼Œåœ¨æ–‡æ¡£å‰é¢çš„éƒ¨åˆ†æœç´¢ search_text n ç§»åŠ¨åˆ°åä¸€ä¸ªæ£€ç´¢ç»“æœ N ç§»åŠ¨åˆ°å‰ä¸€ä¸ªæ£€ç´¢ç»“æœ :%s/original/replacement æ£€ç´¢ç¬¬ä¸€ä¸ª â€œoriginalâ€ å­—ç¬¦ä¸²å¹¶å°†å…¶æ›¿æ¢æˆ â€œreplacementâ€ :%s/original/replacement/g æ£€ç´¢å¹¶å°†æ‰€æœ‰çš„ â€œoriginalâ€ æ›¿æ¢ä¸º â€œreplacementâ€ :%s/original/replacement/gc æ£€ç´¢å‡ºæ‰€æœ‰çš„ â€œoriginalâ€ å­—ç¬¦ä¸²ï¼Œä½†åœ¨æ›¿æ¢æˆ â€œreplacementâ€ å‰ï¼Œå…ˆè¯¢é—®æ˜¯å¦æ›¿æ¢ "},{"id":31,"href":"/docs/code/block/matplotlib-show-chinese/","title":"Matplotlibå±•ç¤ºä¸­æ–‡","section":"ä»£ç å—","content":"åœ¨Matplotlibä¸­å±•ç¤ºä¸­æ–‡ï¼Œé˜²æ­¢ä¹±ç \næ ¸å¿ƒä»£ç ï¼š\n1plt.rcParams[\u0026#39;font.sans-serif\u0026#39;]=[\u0026#39;SimHei\u0026#39;] #ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾ 2plt.rcParams[\u0026#39;axes.unicode_minus\u0026#39;] = False #ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå· å®Œæ•´ä»£ç ï¼š\n1import numpy as np 2import matplotlib.pyplot as plt 3from sklearn.linear_model import Lasso 4 5plt.rcParams[\u0026#39;font.sans-serif\u0026#39;]=[\u0026#39;SimHei\u0026#39;] #ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºä¸­æ–‡æ ‡ç­¾ 6plt.rcParams[\u0026#39;axes.unicode_minus\u0026#39;] = False #ç”¨æ¥æ­£å¸¸æ˜¾ç¤ºè´Ÿå· 7 8# ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ® 9np.random.seed(42) 10disciple_count = np.random.randint(50, 200, size=20) 11establishment_years = np.random.randint(1, 100, size=20) 12weapon_types = np.random.randint(1, 10, size=20) 13master_skill = 2 * disciple_count + 1.5 * establishment_years + 3 * weapon_types + np.random.randn(20) * 20 + 100 14 15# æ•°æ®è½¬æ¢ä¸ºäºŒç»´æ•°ç»„ 16X = np.column_stack((disciple_count, establishment_years, weapon_types)) 17y = master_skill 18 19# åˆ›å»ºLassoå›å½’æ¨¡å‹å¹¶è®­ç»ƒ 20lasso_reg = Lasso(alpha=0.1) 21lasso_reg.fit(X, y) 22 23# æ‰“å°æ¨¡å‹å‚æ•° 24print(\u0026#34;æˆªè·:\u0026#34;, lasso_reg.intercept_) 25print(\u0026#34;ç³»æ•°:\u0026#34;, lasso_reg.coef_) 26 27# å¯è§†åŒ–å›å½’å¹³é¢ï¼ˆè¿™é‡Œåªèƒ½å±•ç¤ºä¸¤ä¸ªç‰¹å¾çš„äºŒç»´å¹³é¢å›¾ï¼‰ 28plt.scatter(disciple_count, master_skill, color=\u0026#39;blue\u0026#39;, label=\u0026#39;å®é™…æ•°æ®\u0026#39;) 29plt.plot(disciple_count, lasso_reg.intercept_ + lasso_reg.coef_[0] * disciple_count + lasso_reg.coef_[1] * np.mean(establishment_years), color=\u0026#39;red\u0026#39;, linewidth=2, label=\u0026#39;å›å½’ç›´çº¿\u0026#39;) 30plt.title(\u0026#34;æ­¦ä¾ å°è¯´ä¸­çš„Lassoå›å½’ç¤ºä¾‹\u0026#34;) 31plt.xlabel(\u0026#34;å¼Ÿå­æ•°é‡\u0026#34;) 32plt.ylabel(\u0026#34;æŒé—¨æ­¦åŠŸä¿®ä¸º\u0026#34;) 33plt.legend() 34plt.show() "},{"id":32,"href":"/docs/db/redis/why-redis-fast/","title":"Redisä¸ºå•¥è¿™ä¹ˆå¿«","section":"Redis","content":"\nWhy is Redis Fast?\nRedis is fast for in-memory data storage. Its speed has made it popular for caching, session storage, and real-time analytics. But what gives Redis its blazing speed? Let\u0026rsquo;s explore:\nğ—¥ğ—”ğ— -ğ—•ğ—®ğ˜€ğ—²ğ—± ğ—¦ğ˜ğ—¼ğ—¿ğ—®ğ—´ğ—²\nAt its core, Redis primarily uses main memory for storing data. Accessing data from RAM is orders of magnitude faster than from disk. This is a major reason for Redis\u0026rsquo;s speed.\nHowever, RAM is volatile. To persist data, Redis supports disk snapshots and append-only file logging. This combines RAM\u0026rsquo;s performance with disk\u0026rsquo;s permanence.\nThere is a tradeoff though - recovery from disk is slow. If a Redis instance fails, restarting from disk can be slow compared to failing over to a replica instance fully in memory. So while Redis offers durability via disk, it comes at the cost of slower recovery.\nA better solution is Redis replication. With a synchronized replica kept in memory, failover is instant with no rehydration. This maintains speed and near-instant recovery.\nğ—œğ—¢ ğ— ğ˜‚ğ—¹ğ˜ğ—¶ğ—½ğ—¹ğ—²ğ˜…ğ—¶ğ—»ğ—´ \u0026amp; ğ—¦ğ—¶ğ—»ğ—´ğ—¹ğ—²-ğ˜ğ—µğ—¿ğ—²ğ—®ğ—±ğ—²ğ—± ğ—¥ğ—²ğ—®ğ—±/ğ—ªğ—¿ğ—¶ğ˜ğ—²\nRedis uses an event-driven, single-threaded model for its core operations. A main event loop handles all client requests and data operations sequentially. This single-threaded execution avoids context switching and synchronization overhead typical of multi-threaded systems.\nRedis uses non-blocking I/O to handle multiple connections asynchronously. This allows it to support many client connections with very low overhead,\nRedis does leverage threading in certain areas:\n- Background tasks like taking snapshots. - I/O threads are used for certain operations. - Modules can use threads. - Since Redis 6.0, it supports multi-threaded I/O for network communication, improving performance on multi-core systems.\nRedis also uses pipelining for high throughput. Clients pipeline commands without waiting for each response. This allows more efficient network round trips, boosting overall performance.\nğ—˜ğ—³ğ—³ğ—¶ğ—°ğ—¶ğ—²ğ—»ğ˜ ğ——ğ—®ğ˜ğ—® ğ—¦ğ˜ğ—¿ğ˜‚ğ—°ğ˜ğ˜‚ğ—¿ğ—²ğ˜€\nRedis supports various optimized data structures, from linked lists, zip lists, and skip lists to sets, hashes, and sorted sets, among others. Each is carefully designed for specific use cases for quick and efficient data access.\nOver to you: With Redis now supporting some multi-threading, how should we configure it to fully utilize all the CPU cores of modern hardware when deploying in production?\n"},{"id":33,"href":"/docs/other/how-visa-make-money/","title":"Visaæ˜¯å¦‚ä½•æŒ£é’±çš„","section":"å…¶å®ƒ","content":"\nWhy is the credit card called â€œğ­ğ¡ğ ğ¦ğ¨ğ¬ğ­ ğ©ğ«ğ¨ğŸğ¢ğ­ğšğ›ğ¥ğ product in banksâ€? How does VISA/Mastercard make money?\nThe diagram below shows the economics of the credit card payment flow.\nThe cardholder pays a merchant $100 to buy a product.\nThe merchant benefits from the use of the credit card with higher sales volume, and needs to compensate the issuer and the card network for providing the payment service. The acquiring bank sets a fee with the merchant, called the â€œğ¦ğğ«ğœğ¡ğšğ§ğ­ ğğ¢ğ¬ğœğ¨ğ®ğ§ğ­ ğŸğğ.â€\n3 - 4. The acquiring bank keeps $0.25 as the ğšğœğªğ®ğ¢ğ«ğ¢ğ§ğ  ğ¦ğšğ«ğ¤ğ®ğ©, and $1.75 is paid to the issuing bank as the ğ¢ğ§ğ­ğğ«ğœğ¡ğšğ§ğ ğ ğŸğğ. The merchant discount fee should cover the interchange fee.\nThe interchange fee is set by the card network because it is less efficient for each issuing bank to negotiate fees with each merchant.\nThe card network sets up the ğ§ğğ­ğ°ğ¨ğ«ğ¤ ğšğ¬ğ¬ğğ¬ğ¬ğ¦ğğ§ğ­ğ¬ ğšğ§ğ ğŸğğğ¬ with each bank, which pays the card network for its services every month. For example, VISA charges a 0.11% assessment, plus a $0.0195 usage fee, for every swipe.\nThe cardholder pays the issuing bank for its services.\nWhy should the issuing bank be compensated? ğŸ”¹The issuer pays the merchant even if the cardholder fails to pay the issuer. ğŸ”¹The issuer pays the merchant before the cardholder pays the issuer. ğŸ”¹The issuer has other operating costs, including managing customer accounts, providing statements, fraud detection, risk management, clearing \u0026amp; settlement, etc.\nOver to you: Does the card network charge the same interchange fee for big merchants as for small merchants?\n"},{"id":34,"href":"/docs/code/front/","title":"å‰ç«¯","section":"ç¼–ç¨‹","content":" ç›¸å…³åœ°å€ # npmä»“åº“åœ°å€\nhttps://www.npmjs.com/~lucumt\ntaobao-npmä»“åº“åœ°å€\nhttps://npmmirror.com/\nç›¸å…³æŒ‡ä»¤ # 1# å‘å¸ƒç‰ˆæœ¬ 2npm publish 3 4# é…ç½®ç›¸å…³ 5npm config list 6npm config delete registry "},{"id":35,"href":"/docs/code/front/fronted-performance-cheatsheet/","title":"å¦‚ä½•æé«˜å‰ç«¯ç½‘ç«™åŠ è½½é€Ÿåº¦","section":"å‰ç«¯","content":"\nHow to load your websites at lightning speed?\nCheck out these 8 tips to boost frontend performance:\nCompression Compress files and minimize data size before transmission to reduce network load.\nSelective Rendering/Windowing Display only visible elements to optimize rendering performance. For example, in a dynamic list, only render visible items.\nModular Architecture with Code Splitting Split a bigger application bundle into multiple smaller bundles for efficient loading.\nPriority-Based Loading Prioritize essential resources and visible (or above-the-fold) content for a better user experience.\nPre-loading Fetch resources in advance before they are requested to improve loading speed.\nTree Shaking or Dead Code Removal Optimize the final JS bundle by removing dead code that will never be used.\nPre-fetching Proactively fetch or cache resources that are likely to be needed soon.\nDynamic Imports Load code modules dynamically based on user actions to optimize the initial loading times.\nOver to you: What other frontend performance tips would you add to this cheat sheet?\n"},{"id":36,"href":"/docs/code/theory/concurrency-is-not-parallelism/","title":"å¹¶å‘ä¸æ˜¯å¹¶è¡Œ","section":"ç†è®º","content":"\nThings Every Developer Should Know: Concurrency is ğğğ“ parallelism.\nIn system design, it is important to understand the difference between concurrency and parallelism.\nAs Rob Pyke(one of the creators of GoLang) stated:â€œ Concurrency is about ğğğšğ¥ğ¢ğ§ğ  ğ°ğ¢ğ­ğ¡ lots of things at once. Parallelism is about ğğ¨ğ¢ğ§ğ  lots of things at once.\u0026quot; This distinction emphasizes that concurrency is more about the ğğğ¬ğ¢ğ ğ§ of a program, while parallelism is about the ğğ±ğğœğ®ğ­ğ¢ğ¨ğ§.\nConcurrency is about dealing with multiple things at once. It involves structuring a program to handle multiple tasks simultaneously, where the tasks can start, run, and complete in overlapping time periods, but not necessarily at the same instant.\nConcurrency is about the composition of independently executing processes and describes a program\u0026rsquo;s ability to manage multiple tasks by making progress on them without necessarily completing one before it starts another.\nParallelism, on the other hand, refers to the simultaneous execution of multiple computations. It is the technique of running two or more tasks or computations at the same time, utilizing multiple processors or cores within a computer to perform several operations concurrently. Parallelism requires hardware with multiple processing units, and its primary goal is to increase the throughput and computational speed of a system.\nIn practical terms, concurrency enables a program to remain responsive to input, perform background tasks, and handle multiple operations in a seemingly simultaneous manner, even on a single-core processor. It\u0026rsquo;s particularly useful in I/O-bound and high-latency operations where programs need to wait for external events, such as file, network, or user interactions.\nParallelism, with its ability to perform multiple operations at the same time, is crucial in CPU-bound tasks where computational speed and throughput are the bottlenecks. Applications that require heavy mathematical computations, data analysis, image processing, and real-time processing can significantly benefit from parallel execution.\n"},{"id":37,"href":"/docs/microservice/develop/microservice-best-practices/","title":"å¾®æœåŠ¡æœ€ä½³å®è·µ","section":"å¾®æœåŠ¡å¼€å‘","content":"\n"},{"id":38,"href":"/docs/db/theory/how-does-search-engine-work/","title":"æœç´¢å¼•æ“å¦‚ä½•å·¥ä½œ","section":"ç†è®º","content":"\nThe diagram below shows a high-level walk-through of a search engine.\nâ–¶ï¸ Step 1 - Crawling Web Crawlers scan the internet for web pages. They follow the URL links from one page to another and store URLs in the URL store. The crawlers discover new content, including web pages, images, videos, and files.\nâ–¶ï¸ Step 2 - Indexing Once a web page is crawled, the search engine parses the page and indexes the content found on the page in a database. The content is analyzed and categorized. For example, keywords, site quality, content freshness, and many other factors are assessed to understand what the page is about.\nâ–¶ï¸ Step 3 - Ranking Search engines use complex algorithms to determine the order of search results. These algorithms consider various factors, including keywords, pages\u0026rsquo; relevance, content quality, user engagement, page load speed, and many others. Some search engines also personalize results based on the user\u0026rsquo;s past search history, location, device, and other personal factors.\nâ–¶ï¸ Step 4 - Querying When a user performs a search, the search engine sifts through its index to provide the most relevant results.\n"},{"id":39,"href":"/docs/linux/system/linux-file-permissions/","title":"æ–‡ä»¶æƒé™è¯´æ˜","section":"Linuxç³»ç»Ÿ","content":"\n"},{"id":40,"href":"/docs/algorithm/12-algorithms-for-system-design-interviews/","title":"ç³»ç»Ÿè®¾è®¡é¢è¯•ä¸­çš„12ç§ç®—æ³•","section":"ç®—æ³•","content":"12 Algorithms for System Design Interviews\nBloom Filter: Check if a requested item is in the cache before performing disk operations. Geohash: Used to build location-based services. HyperLogLog: Counting unique queries performed by users in a search. Consistent Hashing: Used for efficient data distribution between the clusterâ€™s nodes. Merkle Tree: Used to detect inconsistencies between data replicas across multiple nodes. Raft Algorithm: Used to achieve consensus on log replication. Lossy Count: Network traffic analysis, web analytics, and identifying heavy hitters. QuadTree: Used to build location-based services. Operational Transformation: Used to support collaborative editing systems. Leaky Bucket: Used for rate limiting Rsync: Synchronizing files and directories between two different systems. Ray Casting: Used for geospatial analysis, collision detection in video games, and computer graphics. "},{"id":41,"href":"/docs/ai/top-20-ai-concepts/","title":"20ä¸ªä¸»è¦çš„aiæ¦‚å¿µ","section":"AI","content":"\nAI Concepts Developers Should Learn\nMachine Learning: Core algorithms, statistics, and model training techniques. Deep Learning: Neural networks with multiple layers that learn complex patterns automatically. Neural Networks: Layered structures that model complex, nonlinear relationships in data. NLP: Techniques for processing and understanding human language in text form. Computer Vision: Algorithms that interpret and analyze images and visual data. Reinforcement Learning: Training agents through rewards and penalties in interactive environments. Generative Models: AI systems that create new data based on learned patterns. LLM: Large language models that generate human-like text from training data. Transformers: Self-attention architecture that powers most modern AI language models. Feature Engineering: Selecting and designing input features to improve model performance. Unsupervised Learning: Finding patterns in data without using labeled examples. Bayesian Learning: Using probabilistic methods to incorporate uncertainty into models. Prompt Engineering: Crafting effective inputs to guide AI model outputs optimally. AI Agents: Autonomous systems that observe environments and take intelligent actions. Fine-Tuning Models: Adapting pre-trained models for specific tasks and domains. Multimodal Models: AI systems that process text, images, audio, and video together. Embeddings: Converting data into numerical vectors that machines can process effectively. Vector Search: Finding similar items by comparing their mathematical vector representations. Model Evaluation: Testing and measuring how well AI models perform on tasks. AI Infrastructure: Building scalable systems to deploy and run AI applications. "},{"id":42,"href":"/docs/db/theory/top-6-data-manage-patterns/","title":"6ç§æ•°æ®ç®¡ç†æ¨¡å¼","section":"ç†è®º","content":"\nHow do we manage data? Here are top 6 data management patterns.\nğŸ”¹ Cache Aside When an application needs to access data, it first checks the cache. If the data is not present (a cache miss), it fetches the data from the data store, stores it in the cache, and then returns the data to the user. This pattern is particularly useful for scenarios where data is read frequently but updated less often.\nğŸ”¹ Materialized View A Materialized View is a database object that contains the results of a query. It is physically stored, meaning the data is actually computed and stored on disk, as opposed to being dynamically generated upon each request. This can significantly speed up query times for complex calculations or aggregations that would otherwise need to be computed on the fly. Materialized views are especially beneficial in data warehousing and business intelligence scenarios where query performance is critical.\nğŸ”¹ CQRS CQRS is an architectural pattern that separates the models for reading and writing data. This means that the data structures used for querying data (reads) are separated from the structures used for updating data (writes). This separation allows for optimization of each operation independently, improving performance, scalability, and security. CQRS can be particularly useful in complex systems where the read and write operations have very different requirements.\nğŸ”¹ Event Sourcing Event Sourcing is a pattern where changes to the application state are stored as a sequence of events. Instead of storing just the current state of data in a domain, Event Sourcing stores a log of all the changes (events) that have occurred over time. This allows the application to reconstruct past states and provides an audit trail of changes. Event Sourcing is beneficial in scenarios requiring complex business transactions, auditability, and the ability to rollback or replay events.\nğŸ”¹ Index Table The Index Table pattern involves creating additional tables in a database that are optimized for specific query operations. These tables act as secondary indexes and are designed to speed up the retrieval of data without requiring a full scan of the primary data store. Index tables are particularly useful in scenarios with large datasets and where certain queries are performed frequently.\nğŸ”¹ Sharding Sharding is a data partitioning pattern where data is divided into smaller, more manageable pieces, or \u0026ldquo;shards\u0026rdquo;, each of which can be stored on different database servers. This pattern is used to distribute the data across multiple machines to improve scalability and performance. Sharding is particularly effective in high-volume applications, as it allows for horizontal scaling, spreading the load across multiple servers to handle more users and transactions.\n"},{"id":43,"href":"/docs/network/https-handshake/","title":"HTTPSè¿æ¥å»ºç«‹è¿‡ç¨‹","section":"ç½‘ç»œ","content":"\n"},{"id":44,"href":"/docs/network/https-under-the-hood/","title":"HTTPSè¿æ¥è¿‡ç¨‹æ­ç§˜","section":"ç½‘ç»œ","content":"\nHTTPS Under the Hood\nHTTPS is a secure way to share information on the internet. It encrypts data transfer between client and server.\nBut without common encryption key, how data is encrypted?\n1 - Server Certificate Check - Client and server exchange \u0026ldquo;HELLO\u0026rdquo; messages - Server sends its certificate - Client verifies it with a Certificate Authority\n2 - Key Exchange - Client extracts server\u0026rsquo;s public key, creates a session key - They agree on a cipher suite - Client encrypts session key using server\u0026rsquo;s public key - Server decrypts it\n3 - Encrypted Tunnel for data transmission - Client and server both have a common key (session key) - They use it to encrypt and decrypt data during transmission\nThis creates a secure, encrypted tunnel for data transfer, protecting information from eavesdropping and tampering.\n"},{"id":45,"href":"/docs/linux/system/linux-boot-process-explained/","title":"Linuxå¯åŠ¨è¿‡ç¨‹è¯¦è§£","section":"Linuxç³»ç»Ÿ","content":"\nLinux Boot Process Illustrated\nThe diagram below shows the steps.\nStep 1 - When we turn on the power, BIOS (Basic Input/Output System) or UEFI (Unified Extensible Firmware Interface) firmware is loaded from non-volatile memory, and executes POST (Power On Self Test).\nStep 2 - BIOS/UEFI detects the devices connected to the system, including CPU, RAM, and storage.\nStep 3 - Choose a booting device to boot the OS from. This can be the hard drive, the network server, or CD ROM.\nStep 4 - BIOS/UEFI runs the boot loader (GRUB), which provides a menu to choose the OS or the kernel functions.\nStep 5 - After the kernel is ready, we now switch to the user space. The kernel starts up systemd as the first user-space process, which manages the processes and services, probes all remaining hardware, mounts filesystems, and runs a desktop environment.\nStep 6 - systemd activates the default. target unit by default when the system boots. Other analysis units are executed as well.\nStep 7 - The system runs a set of startup scripts and configure the environment.\nStep 8 - The users are presented with a login window. The system is now ready.\n"},{"id":46,"href":"/docs/code/mq/the-core-reddit-architecture/","title":"Redditæ ¸å¿ƒæ¶æ„","section":"æ¶ˆæ¯ä¸­é—´ä»¶","content":"\nA quick look at Redditâ€™s Core Architecture that helps it serve over 1 billion users every month.\nThis information is based on research from many Reddit engineering blogs. But since architecture is ever-evolving, things might have changed in some aspects.\nThe main points of Redditâ€™s architecture are as follows:\nReddit uses a Content Delivery Network (CDN) from Fastly as a front for the application\nReddit started using jQuery in early 2009. Later on, they started using Typescript and have now moved to modern Node.js frameworks. Over the years, Reddit has also built mobile apps for Android and iOS.\nWithin the application stack, the load balancer sits in front and routes incoming requests to the appropriate services.\nReddit started as a Python-based monolithic application but has since started moving to microservices built using Go.\nReddit heavily uses GraphQL for its API layer. In early 2021, they started moving to GraphQL Federation, which is a way to combine multiple smaller GraphQL APIs known as Domain Graph Services (DGS). In 2022, the GraphQL team at Reddit added several new Go subgraphs for core Reddit entities thereby splitting the GraphQL monolith.\nFrom a data storage point of view, Reddit relies on Postgres for its core data model. To reduce the load on the database, they use memcached in front of Postgres. Also, they use Cassandra quite heavily for new features mainly because of its resiliency and availability properties.\nTo support data replication and maintain cache consistency, Reddit uses Debezium to run a Change Data Capture process.\nExpensive operations such as a user voting or submitting a link are deferred to an async job queue via RabbitMQ and processed by job workers. For content safety checks and moderation, they use Kafka to transfer data in real-time to run rules over them.\nReddit uses AWS and Kubernetes as the hosting platform for its various apps and internal services.\nFor deployment and infrastructure, they use Spinnaker, Drone CI, and Terraform.\nOver to you: what other aspects do you know about Redditâ€™s architecture?\n"},{"id":47,"href":"/docs/security/session-vs-jwt/","title":"Sessionå’ŒJWTçš„å¯¹æ¯”","section":"å®‰å…¨","content":"\nWhatâ€™s the difference between Session-based authentication and JWTs?\nHereâ€™s a simple breakdown for both approaches:\nSession-Based Authentication\nIn this approach, you store the session information in a database or session store and hand over a session ID to the user.\nThink of it like a passenger getting just the Ticket ID of their flight while all other details are stored in the airlineâ€™s database.\nHereâ€™s how it works:\n1 - The user makes a login request and the frontend app sends the request to the backend server.\n2 - The backend creates a session using a secret key and stores the data in session storage.\n3 - The server sends a cookie back to the client with the unique session ID.\n4 - The user makes a new request and the browser sends the session ID along with the request.\n5 - The server authenticates the user using the session ID.\nJWT-Based Authentication\nIn the JWT-based approach, you donâ€™t store the session information in the session store.\nThe entire information is available within the token.\nThink of it like getting the flight ticket along with all the details available on the ticket but encoded.\nHereâ€™s how it works:\n1 - The user makes a login request and it goes to the backend server.\n2 - The server verifies the credentials and issues a JWT. The JWT is signed using a private key and no session storage is involved.\n3 - The JWT is passed to the client, either as a cookie or in the response body. Both approaches have their pros and cons but weâ€™ve gone with the cookie approach.\n4 - For every subsequent request, the browser sends the cookie with the JWT.\n5 - The server verifies the JWT using the secret private key and extracts the user info.\n"},{"id":48,"href":"/docs/cloud/cloud-cost-reduction-techniques/","title":"äº‘è®¡ç®—æˆæœ¬é™ä½æŠ€æœ¯","section":"äº‘åŸç”Ÿ","content":"\nIrrational Cloud Cost is the biggest challenge many organizations are battling as they navigate the complexities of cloud computing.\nEfficiently managing these costs is crucial for optimizing cloud usage and maintaining financial health.\nThe following techniques can help businesses effectively control and minimize their cloud expenses.\nReduce Usage: Fine-tune the volume and scale of resources to ensure efficiency without compromising on the performance of applications (e.g., downsizing instances, minimizing storage space, consolidating services).\nTerminate Idle Resources: Locate and eliminate resources that are not in active use, such as dormant instances, databases, or storage units.\nRight Sizing: Adjust instance sizes to adequately meet the demands of your applications, ensuring neither underuse nor overuse.\nShutdown Resources During Off-Peak Times: Set up automatic mechanisms or schedules for turning off non-essential resources when they are not in use, especially during low-activity periods.\nReserve to Reduce Rate: Adopt cost-effective pricing models like Reserved Instances or Savings Plans that align with your specific workload needs.\nBonus Tip: Consider using Spot Instances and lower-tier storage options for additional cost savings.\nOptimize Data Transfers: Utilize methods such as data compression and Content Delivery Networks (CDNs) to cut down on bandwidth expenses, and strategically position resources to reduce data transfer costs, focusing on intra-region transfers.\n"},{"id":49,"href":"/docs/microservice/develop/9-best-practices-for-building-microservices/","title":"å¾®æœåŠ¡å¼€å‘çš„9ä¸ªæœ€ä½³å®è·µ","section":"å¾®æœåŠ¡å¼€å‘","content":"\nCreating a system using microservices is extremely difficult unless you follow some strong principles.\n9 best practices that you must know before building microservices:\n1 - Design For Failure A distributed system with microservices is going to fail.\nYou must design the system to tolerate failure at multiple levels such as infrastructure, database, and individual services. Use circuit breakers, bulkheads, or graceful degradation methods to deal with failures.\n2 - Build Small Services A microservice should not do multiple things at once. A good microservice is designed to do one thing well.\n3 - Use lightweight protocols for communication Communication is the core of a distributed system. Microservices must talk to each other using lightweight protocols. Options include REST, gRPC, or message brokers.\n4 - Implement service discovery To communicate with each other, microservices need to discover each other over the network. Implement service discovery using tools such as Consul, Eureka, or Kubernetes Services\n5 - Data Ownership In microservices, data should be owned and managed by the individual services. The goal should be to reduce coupling between services so that they can evolve independently.\n6 - Use resiliency patterns Implement specific resiliency patterns to improve the availability of the services. Examples: retry policies, caching, and rate limiting.\n7 - Security at all levels In a microservices-based system, the attack surface is quite large. You must implement security at every level of the service communication path.\n8 - Centralized logging Logs are important to finding issues in a system. With multiple services, they become critical.\n9 - Use containerization techniques\nTo deploy microservices in an isolated manner, use containerization techniques.\nTools like Docker and Kubernetes can help with this as they are meant to simplify the scaling and deployment of a microservice.\n"},{"id":50,"href":"/docs/algorithm/big-o-notation-101/","title":"ç®—æ³•æ—¶é—´å¤æ‚åº¦è¯´æ˜","section":"ç®—æ³•","content":" Big O Notation 101: The Secret to Writing Efficient Algorithms\nFrom simple array operations to complex sorting algorithms, understanding the Big O Notation is critical for building high-performance software solutions.\n1 - O(1) This is the constant time notation. The runtime remains steady regardless of input size. For example, accessing an element in an array by index and inserting/deleting an element in a hash table.\n2 - O(n) Linear time notation. The runtime grows in direct proportion to the input size. For example, finding the max or min element in an unsorted array.\n3 - O(log n) Logarithmic time notation. The runtime increases slowly as the input grows. For example, a binary search on a sorted array and operations on balanced binary search trees.\n4 - O(n^2) Quadratic time notation. The runtime grows exponentially with input size. For example, simple sorting algorithms like bubble sort, insertion sort, and selection sort.\n5 - O(n^3) Cubic time notation. The runtime escalates rapidly as the input size increases. For example, multiplying two dense matrices using the naive algorithm.\n6 - O(n logn) Linearithmic time notation. This is a blend of linear and logarithmic growth. For example, efficient sorting algorithms like merge sort, quick sort, and heap sort\n7 - O(2^n) Exponential time notation. The runtime doubles with each new input element. For example, recursive algorithms solve problems by dividing them into multiple subproblems.\n8 - O(n!) Factorial time notation. Runtime skyrockets with input size. For example, permutation-generation problems.\n9 - O(sqrt(n)) Square root time notation. Runtime increases relative to the inputâ€™s square root. For example, searching within a range such as the Sieve of Eratosthenes for finding all primes up to n.\n"},{"id":51,"href":"/docs/code/theory/design-effective-and-safe-apis/","title":"ç¼–å†™å‡ºå®‰å…¨æœ‰æ•ˆçš„API","section":"ç†è®º","content":"\nNote that API design is not just URL path design. Most of the time, we need to choose the proper resource names, identifiers, and path patterns. It is equally important to design proper HTTP header fields or to design effective rate-limiting rules within the API gateway.\n"},{"id":52,"href":"/docs/monitor/top-9-website-performance-metrics/","title":"ç½‘ç«™çš„9å¤§æ€§èƒ½æŒ‡æ ‡","section":"è¿ç»´","content":"\nTop 9 website performance metrics you cannot ignore:\nLoad Time: This is the time taken by the web browser to download and display the webpage. Itâ€™s measured in milliseconds.\nTime to First Byte (TTFB): Itâ€™s the time taken by the browser to receive the first byte of data from the web server. TTFB is crucial because it indicates the general ability of the server to handle traffic.\nRequest Count: The number of HTTP requests a browser has to make to fully load the page. The lower this count, the faster a website will feel to the user.\nDOMContentLoaded (DCL): This is the time it takes for the full HTML code of a webpage to be loaded. The faster this happens, the faster users can see useful functionality. This time doesnâ€™t include loading CSS and other assets\nTime to above-the-fold load: â€œAbove the foldâ€ is the area of a webpage that fits in a browser window without a user having to scroll down. This is the content that is first seen by the user and often dictates whether theyâ€™ll continue reading the webpage.\nFirst Contentful Paint (FCP): This is the time at which content first begins to be â€œpaintedâ€ by the browser. It can be a text, image, or even background color.\nPage Size: This is the total file size of all content and assets that appear on the page. Over the last several years, the page size of websites has been growing constantly. The bigger the size of a webpage, the longer it will take to load\nRound Trip Time (RTT): This is the amount of time a round trip takes. A round trip constitutes a request traveling from the browser to the origin server and the response from the server going to the browser. Reducing RTT is one of the key approaches to improving a websiteâ€™s performance.\nRender Blocking Resources: Some resources block other parts of the page from being loaded. Itâ€™s important to track the number of such resources. The more render-blocking resources a webpage has, the greater the delay for the browser to load the page.\n"},{"id":53,"href":"/docs/code/block/python-generate-site-url/","title":"é€šè¿‡è„šæœ¬ç”Ÿæˆç½‘ç«™URLåˆ—è¡¨","section":"ä»£ç å—","content":"Pythonè„šæœ¬ç”¨äºå¯¹ç½‘ç«™çš„sitemap.xmlæ–‡ä»¶è¿›è¡Œè§£æ\n1from lxml import etree 2import requests, os 3 4if __name__ == \u0026#34;__main__\u0026#34;: 5 6 file_path = r\u0026#34;e:\\urls.txt\u0026#34; 7 try: 8 os.remove(file_path) 9 except OSError: 10 pass 11 12 with open(file_path, \u0026#39;a\u0026#39;) as url_file: 13 xml_dict = {} 14 15 r = requests.get(\u0026#34;https://lucumt.info/sitemap.xml\u0026#34;) 16 root = etree.fromstring(r.content) 17 count = 0 18 for sitemap in root: 19 data = sitemap.getchildren()[0].text 20 if \u0026#39;tags\u0026#39; in data or \u0026#39;categories\u0026#39; in data: 21 continue 22 count = count + 1 23 url_file.write(data + \u0026#39;\\n\u0026#39;) 24 print(f\u0026#34;Url files write success,total count {count}\u0026#34;) ä½¿ç”¨ç”Ÿæˆçš„urls.txtæ–‡ä»¶åˆ©ç”¨curlå‘½ä»¤æäº¤ç™¾åº¦æ”¶å½•\n1curl -H \u0026#39;Content-Type:text/plain\u0026#39; --data-binary @urls.txt \u0026#34;http://data.zz.baidu.com/urls?site=https://lucumt.info\u0026amp;token=xxx\u0026#34; "},{"id":54,"href":"/docs/monitor/top-6-heartbeat-detection-mechanisms/","title":"6ç§å¿ƒè·³æ£€æµ‹æœºåˆ¶","section":"è¿ç»´","content":"\nHow do we detect node failures in distributed systems? . . The diagram below shows top 6 Heartbeat Detection Mechanisms.\nHeartbeat mechanisms are crucial in distributed systems for monitoring the health and status of various components. Here are several types of heartbeat detection mechanisms commonly used in distributed systems:\nğŸ”¹ Push-Based Heartbeat The most basic form of heartbeat involves a periodic signal sent from one node to another or to a monitoring service. If the heartbeat signals stop arriving within a specified interval, the system assumes that the node has failed. This is simple to implement, but network congestion can lead to false positives.\nğŸ”¹ Pull-Based Heartbeat Instead of nodes sending heartbeats actively, a central monitor might periodically \u0026ldquo;pull\u0026rdquo; status information from nodes. It reduces network traffic but might increase latency in failure detection.\nğŸ”¹ Heartbeat with Health Check This includes diagnostic information about the node\u0026rsquo;s health in the heartbeat signal. This information can include CPU usage, memory usage, or application-specific metrics. It Provides more detailed information about the node, allowing for more nuanced decision-making. However, it Increases complexity and potential for larger network overhead.\nğŸ”¹ Heartbeat with Timestamps Heartbeats that include timestamps can help the receiving node or service determine not just if a node is alive, but also if there are network delays affecting communication.\nğŸ”¹ Heartbeat with Acknowledgement The receiver of the heartbeat message must send back an acknowledgment in this model. This ensures that not only is the sender alive, but the network path between the sender and receiver is also functional.\nğŸ”¹ Heartbeat with Quorum In some distributed systems, especially those involving consensus protocols like Paxos or Raft, the concept of a quorum (a majority of nodes) is used. Heartbeats might be used to establish or maintain a quorum, ensuring that a sufficient number of nodes are operational for the system to make decisions. This brings complexity in implementation and managing quorum changes as nodes join or leave the system.\n"},{"id":55,"href":"/docs/db/theory/top-6-database-models/","title":"6ç§æ•°æ®æ¨¡å‹","section":"ç†è®º","content":"\nThe diagram below shows top 6 data models.\nğŸ”¹ Flat Model The flat data model is one of the simplest forms of database models. It organizes data into a single table where each row represents a record and each column represents an attribute. This model is similar to a spreadsheet and is straightforward to understand and implement. However, it lacks the ability to efficiently handle complex relationships between data entities.\nğŸ”¹ Hierarchical Model The hierarchical data model organizes data into a tree-like structure, where each record has a single parent but can have multiple children. This model is efficient for scenarios with a clear \u0026ldquo;parent-child\u0026rdquo; relationship among data entities. However, it struggles with many-to-many relationships and can become complex and rigid.\nğŸ”¹ Relational Model Introduced by E.F. Codd in 1970, the relational model represents data in tables (relations), consisting of rows (tuples) and columns (attributes). It supports data integrity and avoids redundancy through the use of keys and normalization. The relational model\u0026rsquo;s strength lies in its flexibility and the simplicity of its query language, SQL (Structured Query Language), making it the most widely used data model for traditional database systems. It efficiently handles many-to-many relationships and supports complex queries and transactions.\nğŸ”¹ Star Schema The star schema is a specialized data model used in data warehousing for OLAP (Online Analytical Processing) applications. It features a central fact table that contains measurable, quantitative data, surrounded by dimension tables that contain descriptive attributes related to the fact data. This model is optimized for query performance in analytical applications, offering simplicity and fast data retrieval by minimizing the number of joins needed for queries.\nğŸ”¹ Snowflake Model The snowflake model is a variation of the star schema where the dimension tables are normalized into multiple related tables, reducing redundancy and improving data integrity. This results in a structure that resembles a snowflake. While the snowflake model can lead to more complex queries due to the increased number of joins, it offers benefits in terms of storage efficiency and can be advantageous in scenarios where dimension tables are large or frequently updated.\nğŸ”¹ Network Model The network data model allows each record to have multiple parents and children, forming a graph structure that can represent complex relationships between data entities. This model overcomes some of the hierarchical model\u0026rsquo;s limitations by efficiently handling many-to-many relationships.\n"},{"id":56,"href":"/docs/network/how-https-works/","title":"HTTPSæ˜¯å¦‚ä½•å·¥ä½œçš„","section":"ç½‘ç»œ","content":"\nHTTPS: Safeguards your data from eavesdroppers and breaches. Understand how encryption and digital certificates create an impregnable shield.\nSSL Handshake: Behind the Scenes â€” Witness the cryptographic protocols that establish a secure connection. Experience the intricate exchange of keys and negotiation.\nSecure Data Transmission: Navigating the Tunnel â€” Journey through the encrypted tunnel forged by HTTPS. Learn how your information travels while shielded from cyber threats.\nHTML\u0026rsquo;s Role: Peek into HTML\u0026rsquo;s role in structuring the web. Uncover how hyperlinks and content come together seamlessly. And why is it called HYPER TEXT.\n"},{"id":57,"href":"/docs/code/mq/kafka-101/","title":"Kafkaæ ¸å¿ƒæ¦‚å¿µ","section":"æ¶ˆæ¯ä¸­é—´ä»¶","content":"\nThe Ultimate Kafka 101 You Cannot Miss\nKafka is super-popular but can be overwhelming in the beginning.\nHere are 8 simple steps that can help you understand the fundamentals of Kafka.\n1 - What is Kafka? Kafka is a distributed event store and a streaming platform. It began as an internal project at LinkedIn and now powers some of the largest data pipelines in the world in orgs like Netflix, Uber, etc.\n2 - Kafka Messages Message is the basic unit of data in Kafka. Itâ€™s like a record in a table consisting of headers, key, and value.\n3 - Kafka Topics and Partitions Every message goes to a particular Topic. Think of the topic as a folder on your computer. Topics also have multiple partitions.\n4 - Advantages of Kafka Kafka can handle multiple producers and consumers, while providing disk-based data retention and high scalability.\n5 - Kafka Producer Producers in Kafka create new messages, batch them, and send them to a Kafka topic. They also take care of balancing messages across different partitions.\n6 - Kafka Consumer Kafka consumers work together as a consumer group to read messages from the broker.\n7 - Kafka Cluster A Kafka cluster consists of several brokers where each partition is replicated across multiple brokers to ensure high availability and redundancy.\n8 - Use Cases of Kafka Kafka can be used for log analysis, data streaming, change data capture, and system monitoring.\nOver to you: What else would you add to get a better understanding of Kafka?\n"},{"id":58,"href":"/docs/security/oauth-2.0-explained/","title":"Oauth2.0è¯¦è§£","section":"å®‰å…¨","content":"\nOauth 2.0 Explained With Simple Terms.\nOAuth 2.0 is a powerful and secure framework that allows different applications to securely interact with each other on behalf of users without sharing sensitive credentials.\nThe entities involved in OAuth are the User, the Server, and the Identity Provider (IDP).\nWhat Can an OAuth Token Do?\nWhen you use OAuth, you get an OAuth token that represents your identity and permissions. This token can do a few important things:\nSingle Sign-On (SSO): With an OAuth token, you can log into multiple services or apps using just one login, making life easier and safer.\nAuthorization Across Systems: The OAuth token allows you to share your authorization or access rights across various systems, so you don\u0026rsquo;t have to log in separately everywhere.\nAccessing User Profile: Apps with an OAuth token can access certain parts of your user profile that you allow, but they won\u0026rsquo;t see everything.\nRemember, OAuth 2.0 is all about keeping you and your data safe while making your online experiences seamless and hassle-free across different applications and services.\nOver to you: Imagine you have a magical power to grant one wish to OAuth 2.0. What would that be? Maybe your suggestions actually lead to OAuth 3.\n"},{"id":59,"href":"/docs/code/block/shell-check-between-arrays/","title":"Shellæ•°ç»„ä¸­æ¨¡ç³Šæ£€æµ‹","section":"ä»£ç å—","content":"æ£€æµ‹æ•°ç»„aä¸­çš„å…ƒç´ æ˜¯å¦æ¨¡ç³ŠåŒ…å«æ•°ç»„bä¸­çš„ä¸€ä¸ªæˆ–å¤šä¸ªå…ƒç´ \n1#!/bin/bash 2 3# æ•°ç»„aå’Œæ•°ç»„bçš„å®šä¹‰ 4a=(\u0026#34;tainan\u0026#34; \u0026#34;taipei\u0026#34; \u0026#34;taichung\u0026#34; \u0026#34;keelung\u0026#34; \u0026#34;taoyuan\u0026#34; \u0026#34;changhua\u0026#34; \u0026#34;nantou\u0026#34; \u0026#34;kinmen\u0026#34;) 5b=(\u0026#34;tai\u0026#34; \u0026#34;nan\u0026#34;) 6 7 8for element_a in \u0026#34;${a[@]}\u0026#34;; do 9 contains=false 10 for pattern in \u0026#34;${b[@]}\u0026#34;; do 11 if [[ $element_a =~ $pattern ]]; then 12 contains=true 13 break 14 fi 15 done 16 if [ \u0026#34;$contains\u0026#34; = true ] ; then 17 echo \u0026#34;----------$element_aç¬¦åˆè¦æ±‚---------------\u0026#34; 18 else 19 echo \u0026#34;**********$element_aä¸ç¬¦åˆè¦æ±‚************\u0026#34; 20 fi 21done ä¸‹è¿°æ”¹è¿›ç‰ˆçš„ï¼Œå¯é¿å…åŒé‡å¾ªç¯\n1#!/bin/bash 2 3# æ•°ç»„aå’Œæ•°ç»„bçš„å®šä¹‰ 4a=(\u0026#34;tainan\u0026#34; \u0026#34;taipei\u0026#34; \u0026#34;taichung\u0026#34; \u0026#34;keelung\u0026#34; \u0026#34;taoyuan\u0026#34; \u0026#34;changhua\u0026#34; \u0026#34;nantou\u0026#34; \u0026#34;kinmen\u0026#34;) 5b=(\u0026#34;tai\u0026#34; \u0026#34;nan\u0026#34;) 6 7# å°†æ•°ç»„bä¸­çš„å…ƒç´ è¿æ¥æˆæ­£åˆ™è¡¨è¾¾å¼ 8regex=$(IFS=\u0026#39;|\u0026#39;; echo \u0026#34;${b[*]}\u0026#34;) 9 10for element_a in \u0026#34;${a[@]}\u0026#34;; do 11 if echo \u0026#34;$element_a\u0026#34; | egrep -iq \u0026#34;$regex\u0026#34;; then 12 echo \u0026#34;----------$element_aç¬¦åˆè¦æ±‚---------------\u0026#34; 13 else 14 echo \u0026#34;**********$element_aä¸ç¬¦åˆè¦æ±‚************\u0026#34; 15 fi 16done "},{"id":60,"href":"/docs/cloud/create-jenkins-via-docker/","title":"é€šè¿‡Dockeråˆ›å»ºJenkins","section":"äº‘åŸç”Ÿ","content":" 1version: \u0026#39;3\u0026#39; 2services: 3 docker_jenkins: 4 user: root 5 restart: always 6 image: jenkins/jenkins:lts 7 container_name: jenkins_local 8 ports: 9 - 30180:8080 10 - 50000:50000 11 volumes: 12 - $PWD/jenkins_home/:/var/jenkins_home 13 - /var/run/docker.sock:/var/run/docker.sock 14 - /usr/bin/docker:/usr/bin/docker 15 - /usr/local/bin/docker-compose:/usr/local/bin/docker-compose "},{"id":61,"href":"/docs/code/block/java-lambda-filter/","title":"Javaä¸­åˆ©ç”¨Lambdaè¿›è¡Œè¿‡æ»¤","section":"ä»£ç å—","content":" å¤æ‚çš„è¿‡æ»¤ # 1import com.google.common.collect.Lists; 2import lombok.AllArgsConstructor; 3import lombok.Data; 4import org.apache.commons.lang3.StringUtils; 5 6import java.util.List; 7import java.util.stream.Collectors; 8 9public class FilterTest { 10 11 public static void testData() { 12 List\u0026lt;Record\u0026gt; recordsA = Lists.newArrayList( 13 new Record(\u0026#34;A\u0026#34;, \u0026#34;a1\u0026#34;), 14 new Record(\u0026#34;A\u0026#34;, \u0026#34;a2\u0026#34;), 15 new Record(\u0026#34;A\u0026#34;, \u0026#34;a4\u0026#34;), 16 new Record(\u0026#34;A\u0026#34;, \u0026#34;a5\u0026#34;), 17 new Record(\u0026#34;B\u0026#34;, \u0026#34;b1\u0026#34;), 18 new Record(\u0026#34;B\u0026#34;, \u0026#34;b2\u0026#34;), 19 new Record(\u0026#34;B\u0026#34;, \u0026#34;b7\u0026#34;), 20 new Record(\u0026#34;D\u0026#34;, \u0026#34;b7\u0026#34;) 21 ); 22 List\u0026lt;Record\u0026gt; recordsB = Lists.newArrayList( 23 new Record(\u0026#34;A\u0026#34;, \u0026#34;a1\u0026#34;), 24 new Record(\u0026#34;A\u0026#34;, \u0026#34;a2\u0026#34;), 25 new Record(\u0026#34;A\u0026#34;, \u0026#34;a3\u0026#34;), 26 new Record(\u0026#34;B\u0026#34;, \u0026#34;b1\u0026#34;), 27 new Record(\u0026#34;B\u0026#34;, \u0026#34;b2\u0026#34;), 28 new Record(\u0026#34;B\u0026#34;, \u0026#34;b3\u0026#34;), 29 new Record(\u0026#34;B\u0026#34;, \u0026#34;a1\u0026#34;), 30 new Record(\u0026#34;C\u0026#34;, \u0026#34;c1\u0026#34;), 31 new Record(\u0026#34;C\u0026#34;, \u0026#34;c2\u0026#34;), 32 new Record(\u0026#34;C\u0026#34;, \u0026#34;b1\u0026#34;) 33 ); 34 35 // æŸ¥æ‰¾å‡ºæ–°å¢çš„ 36 List\u0026lt;Record\u0026gt; addRecordsA = recordsB.stream().filter(n -\u0026gt; recordsA.stream().noneMatch(o -\u0026gt; StringUtils.equals(o.getCate(), n.getCate()))).collect(Collectors.toList()); 37 List\u0026lt;Record\u0026gt; addRecordsB = recordsB.stream().filter(n -\u0026gt; recordsA.stream().filter(o -\u0026gt; StringUtils.equals(o.getCate(), n.getCate())).noneMatch(o -\u0026gt; StringUtils.equals(o.getProd(), n.getProd()))).collect(Collectors.toList()); 38 List\u0026lt;Record\u0026gt; addRecordsC = recordsB.stream().filter(n -\u0026gt; recordsA.stream().noneMatch(o -\u0026gt; StringUtils.equals(o.getProd(), n.getProd()))).collect(Collectors.toList()); 39 System.out.println(addRecordsA); 40 System.out.println(addRecordsB); // åªæœ‰è¿™ä¸ªç¬¦åˆè¦æ±‚ 41 System.out.println(addRecordsC); 42 43 // æŸ¥æ‰¾åˆ é™¤çš„ 44 List\u0026lt;Record\u0026gt; deleteRecordsB = recordsA.stream().filter(n -\u0026gt; recordsB.stream().filter(o -\u0026gt; StringUtils.equals(o.getCate(), n.getCate())).noneMatch(o -\u0026gt; StringUtils.equals(o.getProd(), n.getProd()))).collect(Collectors.toList()); 45 System.out.println(deleteRecordsB); 46 } 47 48 @Data 49 @AllArgsConstructor 50 public static class Record { 51 private String cate; 52 private String prod; 53 } 54} "},{"id":62,"href":"/docs/code/theory/top-9-system-integrations/","title":"REST API vs GraphQL","section":"ç†è®º","content":"\nREST API Vs. GraphQL When it comes to API design, REST and GraphQL each have their own strengths and weaknesses.\nREST - Uses standard HTTP methods like GET, POST, PUT, DELETE for CRUD operations. - Works well when you need simple, uniform interfaces between separate services/applications. - Caching strategies are straightforward to implement. - The downside is it may require multiple roundtrips to assemble related data from separate endpoints.\nGraphQL - Provides a single endpoint for clients to query for precisely the data they need. - Clients specify the exact fields required in nested queries, and the server returns optimized payloads containing just those fields. - Supports Mutations for modifying data and Subscriptions for real-time notifications. - Great for aggregating data from multiple sources and works well with rapidly evolving frontend requirements. - However, it shifts complexity to the client side and can allow abusive queries if not properly safeguarded - Caching strategies can be more complicated than REST.\nThe best choice between REST and GraphQL depends on the specific requirements of the application and development team. GraphQL is a good fit for complex or frequently changing frontend needs, while REST suits applications where simple and consistent contracts are preferred.\n"},{"id":63,"href":"/docs/security/cross-site-scripting/","title":"XSSæ”»å‡»è¯¦è§£","section":"å®‰å…¨","content":"\nEverything You Need to Know About Cross-Site Scripting (XSS).\nXSS, a prevalent vulnerability, occurs when malicious scripts are injected into web pages, often through input fields. Check out the diagram below for a deeper dive into how this vulnerability emerges when user input is improperly handled and subsequently returned to the client, leaving systems vulnerable to exploitation.\nUnderstanding the distinction between Reflective and Stored XSS is crucial. Reflective XSS involves immediate execution of the injected script, while Stored XSS persists over time, posing long-term threats. Dive into the diagrams for a comprehensive comparison of these attack vectors.\nImagine this scenario: A cunning hacker exploits XSS to clandestinely harvest user credentials, such as cookies, from their browser, potentially leading to unauthorized access and data breaches. It\u0026rsquo;s a chilling reality.\nBut fret not! Our flyer also delves into effective mitigation strategies, empowering you to fortify your systems against XSS attacks. From input validation and output encoding to implementing strict Content Security Policies (CSP), we\u0026rsquo;ve got you covered.\nOver to you: How can we amplify user awareness to proactively prevent falling victim to XSS attacks? Share your insights and strategies below! Let\u0026rsquo;s collaboratively bolster our web defenses and foster a safer digital environment.\n"},{"id":64,"href":"/docs/code/theory/rest-api-vs-graphql/","title":"æ•°æ®é€šä¿¡çš„9ç§æ¶æ„æ¨¡å¼","section":"ç†è®º","content":"\nTop 9 Architectural Patterns for Data and Communication Flow\nğŸ”¹ Peer-to-Peer The Peer-to-Peer pattern involves direct communication between two components without the need for a central coordinator.\nğŸ”¹ API Gateway An API Gateway acts as a single entry point for all client requests to the backend services of an application.\nğŸ”¹ Pub-Sub The Pub-Sub pattern decouples the producers of messages (publishers) from the consumers of messages (subscribers) through a message broker.\nğŸ”¹ Request-Response This is one of the most fundamental integration patterns, where a client sends a request to a server and waits for a response.\nğŸ”¹ Event Sourcing Event Sourcing involves storing the state changes of an application as a sequence of events.\nğŸ”¹ ETL ETL is a data integration pattern used to gather data from multiple sources, transform it into a structured format, and load it into a destination database.\nğŸ”¹ Batching Batching involves accumulating data over a period or until a certain threshold is met before processing it as a single group.\nğŸ”¹ Streaming Processing Streaming Processing allows for the continuous ingestion, processing, and analysis of data streams in real-time.\nğŸ”¹ Orchestration Orchestration involves a central coordinator (an orchestrator) managing the interactions between distributed components or services to achieve a workflow or business process.\n"},{"id":65,"href":"/docs/network/rodamap-for-learning-cyber-security/","title":"ç½‘ç»œå®‰å…¨å­¦ä¹ çº¿è·¯å›¾","section":"ç½‘ç»œ","content":"\nCybersecurity is crucial for protecting information and systems from theft, damage, and unauthorized access. Whether you\u0026rsquo;re a beginner or looking to advance your technical skills, there are numerous resources and paths you can take to learn more about cybersecurity. Here are some structured suggestions to help you get started or deepen your knowledge:\nğŸ”¹ Security Architecture ğŸ”¹ Frameworks \u0026amp; Standards ğŸ”¹ Application Security ğŸ”¹ Risk Assessment ğŸ”¹ Enterprise Risk Management ğŸ”¹ Threat Intelligence ğŸ”¹ Security Operation\n"},{"id":66,"href":"/docs/db/theory/choosing-the-right-database/","title":"é€‰æ‹©åˆé€‚çš„æ•°æ®åº“","section":"ç†è®º","content":"\nChoosing the Right Database\nWhen deciding which type of database to use, it can be overwhelming to choose from the many available options. Here\u0026rsquo;s a brief summary of some common database architectures and their use cases:\nRelational databases: These are versatile and can solve almost any problem. They are suitable for structured data with well-defined relationships between entities.\nIn-memory stores: With their high speed and limited data size, in-memory databases are perfect for applications that require fast operations, such as caching or real-time analytics.\nTime-series databases: Designed to store and manage time-stamped data, these databases are ideal for monitoring systems, IoT applications, and financial trading platforms.\nGraph databases: If the data involves complex relationships between unstructured objects, graph databases could be a good option. They handle highly connected data, such as social networks or recommendation engines.\nDocument stores: These data stores are suitable for storing large, immutable data, such as user profiles, product catalogs, or content management systems with large amounts of audio and video content.\nWide column stores: Typically used for big data and analytics, wide column stores are great for semi-structured data. They are designed for high scalability and performance.\n"},{"id":67,"href":"/docs/code/block/golang-web-server/","title":"åˆ©ç”¨Goç”Ÿæˆç®€æ˜“çš„Web Server","section":"ä»£ç å—","content":"é‡‡ç”¨Golangç”ŸæˆWeb Serveræ— éœ€é¢å¤–å¤„ç†ç›¸å…³çš„é™æ€æ–‡ä»¶ç­‰å¼•ç”¨ï¼Œä¼šå‡å°‘ä»£ç é‡å’Œç»´æŠ¤å·¥ä½œé‡\n1package main 2 3import ( 4\t\u0026#34;encoding/json\u0026#34; 5\t\u0026#34;io/ioutil\u0026#34; 6\t\u0026#34;log\u0026#34; 7\t\u0026#34;net/http\u0026#34; 8\t\u0026#34;os\u0026#34; 9\t\u0026#34;strconv\u0026#34; 10) 11 12type ConfigData struct { 13\tInfo string `json:\u0026#34;info\u0026#34;` 14\tFile string `json:\u0026#34;file\u0026#34;` 15\tPort int `json:\u0026#34;port\u0026#34;` 16} 17 18func parseConfigFile() ConfigData { 19\tjsonFile, err := os.Open(\u0026#34;config.json\u0026#34;) 20\tif err != nil { 21\tlog.Println(err) 22\t} 23\tlog.Println(\u0026#34;Successfully Opened config.json\u0026#34;) 24 25\tdefer jsonFile.Close() 26 27\tbyteValue, _ := ioutil.ReadAll(jsonFile) 28 29\tvar config ConfigData 30\terr = json.Unmarshal(byteValue, \u0026amp;config) 31\tif err != nil { 32\tlog.Fatal(err) 33\t} 34\treturn config 35} 36 37func main() { 38\tvar config = parseConfigFile() 39\tlog.Println(\u0026#34;port: \u0026#34; + strconv.Itoa(config.Port)) 40\tlog.Println(\u0026#34;file: \u0026#34; + config.File) 41\tlog.Println(config.Info) 42\tlog.Println(\u0026#34;å¯é€šè¿‡ http://127.0.0.1:\u0026#34; + strconv.Itoa(config.Port) + \u0026#34; è®¿é—®æ­¤ç³»ç»Ÿ\u0026#34;) 43 44\thttp.Handle(\u0026#34;/\u0026#34;, http.FileServer(http.Dir(config.File))) 45\thttp.ListenAndServe(\u0026#34;:\u0026#34;+strconv.Itoa(config.Port), nil) 46} "},{"id":68,"href":"/docs/db/theory/database-scaling-cheatsheet/","title":"æ‰©å±•æ•°æ®åº“çš„7ç§ç­–ç•¥","section":"ç†è®º","content":"\n7 must-know strategies to scale your database.\n1 - Indexing: Check the query patterns of your application and create the right indexes.\n2 - Materialized Views: Pre-compute complex query results and store them for faster access.\n3 - Denormalization: Reduce complex joins to improve query performance.\n4 - Vertical Scaling Boost your database server by adding more CPU, RAM, or storage.\n5 - Caching Store frequently accessed data in a faster storage layer to reduce database load.\n6 - Replication Create replicas of your primary database on different servers for scaling the reads.\n7 - Sharding Split your database tables into smaller pieces and spread them across servers. Used for scaling the writes as well as the reads.\nOver to you: What other strategies do you use for scaling your databases?\n"},{"id":69,"href":"/docs/security/how-digital-signatures-work/","title":"æ•°å­—ç­¾åå¦‚ä½•å·¥ä½œ","section":"å®‰å…¨","content":"How Digital Signatures Work?\nA digital signature is a specific kind of electronic signature to sign and secure electronically transmitted documents.\nDigital signatures are similar to physical signatures since they are unique to every person. They identify the identity of the signer.\nHereâ€™s an example of the working process of a digital signature with Alice as the sender and John as the recipient:\nAlice generates a cryptographic key pair consisting of a private key and a corresponding public key. The private key remains confidential and is known only to the signer, while the public key can be shared openly. The signer (Alice) uses a hash function to create a unique fixed-length string of numbers and letters, called a hash, from the document. This hash value represents the contents of the document. Alice uses their private key to encrypt the hash value of the message. This hash value is known as the digital signature. The digital signature is attached to the original document, creating a digitally signed document. It is transmitted over the network to the recipient. The recipient (John) extracts both the digital signature and the original hash value from the document. The recipient uses Aliceâ€™s public key to decrypt the digital signature. This produces a hash value that was originally encrypted with the private key. The recipient calculates a new hash value for the received message using the same hashing algorithm as the signer. They then compare this recalculated hash with the decrypted hash value obtained from the digital signature. If the hash values are equal, the digital signature is valid, and it is determined that the document has not been tampered with or altered. "},{"id":70,"href":"/docs/network/what-is-osi-model/","title":"ç½‘ç»œ7å±‚æ¨¡å‹","section":"ç½‘ç»œ","content":"\nHow is data sent over the network? Why do we need so many layers in the OSI model?\nThe diagram below shows how data is encapsulated and de-encapsulated when transmitting over the network.\nğŸ”¹ Step 1: When Device A sends data to Device B over the network via the HTTP protocol, it is first added an HTTP header at the application layer.\nğŸ”¹ Step 2: Then a TCP or a UDP header is added to the data. It is encapsulated into TCP segments at the transport layer. The header contains the source port, destination port, and sequence number.\nğŸ”¹ Step 3: The segments are then encapsulated with an IP header at the network layer. The IP header contains the source/destination IP addresses.\nğŸ”¹ Step 4: The IP datagram is added a MAC header at the data link layer, with source/destination MAC addresses.\nğŸ”¹ Step 5: The encapsulated frames are sent to the physical layer and sent over the network in binary bits.\nğŸ”¹ Steps 6-10: When Device B receives the bits from the network, it performs the de-encapsulation process, which is a reverse processing of the encapsulation process. The headers are removed layer by layer, and eventually, Device B can read the data.\nWe need layers in the network model because each layer focuses on its own responsibilities. Each layer can rely on the headers for processing instructions and does not need to know the meaning of the data from the last layer.\n"},{"id":71,"href":"/docs/code/theory/12-factor-app/","title":"è½¯ä»¶å¼€å‘12å‡†åˆ™","section":"ç†è®º","content":"\nHave you heard of the 12-Factor App?\nThe \u0026ldquo;12 Factor App\u0026rdquo; offers a set of best practices for building modern software applications. Following these 12 principles can help developers and teams in building reliable, scalable, and manageable applications.\nHere\u0026rsquo;s a brief overview of each principle:\nCodebase: Have one place to keep all your code, and manage it using version control like Git.\nDependencies: List all the things your app needs to work properly, and make sure they\u0026rsquo;re easy to install.\nConfig: Keep important settings like database credentials separate from your code, so you can change them without rewriting code.\nBacking Services: Use other services (like databases or payment processors) as separate components that your app connects to.\nBuild, Release, Run: Make a clear distinction between preparing your app, releasing it, and running it in production.\nProcesses: Design your app so that each part doesn\u0026rsquo;t rely on a specific computer or memory. It\u0026rsquo;s like making LEGO blocks that fit together.\nPort Binding: Let your app be accessible through a network port, and make sure it doesn\u0026rsquo;t store critical information on a single computer.\nConcurrency: Make your app able to handle more work by adding more copies of the same thing, like hiring more workers for a busy restaurant.\nDisposability: Your app should start quickly and shut down gracefully, like turning off a light switch instead of yanking out the power cord.\nDev/Prod Parity: Ensure that what you use for developing your app is very similar to what you use in production, to avoid surprises.\nLogs: Keep a record of what happens in your app so you can understand and fix issues, like a diary for your software.\nAdmin Processes: Run special tasks separately from your app, like doing maintenance work in a workshop instead of on the factory floor.\n"},{"id":72,"href":"/docs/code/theory/10-good-coding-principle/","title":"10ä¸ªè‰¯å¥½ç¼–ç å‡†åˆ™","section":"ç†è®º","content":"\nSoftware development requires good system designs and coding standards. We list 10 good coding principles in the diagram below.\nğŸ”¹ 01 Follow Code Specifications When we write code, it is important to follow the industry\u0026rsquo;s well-established norms, like â€œPEP 8â€, â€œGoogle Java Styleâ€, adhering to a set of agreed-upon code specifications ensures that the quality of the code is consistent and readable.\nğŸ”¹ 02 Documentation and Comments Good code should be clearly documented and commented to explain complex logic and decisions, and comments should explain why a certain approach was taken (â€œWhyâ€) rather than what exactly is being done (â€œWhatâ€). Documentation and comments should be clear, concise, and continuously updated.\nğŸ”¹ 03 Robustness Good code should be able to handle a variety of unexpected situations and inputs without crashing or producing unpredictable results. Most common approach is to catch and handle exceptions.\nğŸ”¹ 04 Follow the SOLID principle â€œSingle Responsibilityâ€, â€œOpen/Closedâ€, â€œLiskov Substitutionâ€, â€œInterface Segregationâ€, and â€œDependency Inversionâ€ - these five principles (SOLID for short) are the cornerstones of writing code that scales and is easy to maintain.\nğŸ”¹ 05 Make Testing Easy Testability of software is particularly important. Good code should be easy to test, both by trying to reduce the complexity of each component, and by supporting automated testing to ensure that it behaves as expected.\nğŸ”¹ 06 Abstraction Abstraction requires us to extract the core logic and hide the complexity, thus making the code more flexible and generic. Good code should have a moderate level of abstraction, neither over-designed nor neglecting long-term expandability and maintainability.\nğŸ”¹ 07 Utilize Design Patterns, but don\u0026rsquo;t over-design Design patterns can help us solve some common problems. However, every pattern has its applicable scenarios. Overusing or misusing design patterns may make your code more complex and difficult to understand.\nğŸ”¹ 08 Reduce Global Dependencies We can get bogged down in dependencies and confusing state management if we use global variables and instances. Good code should rely on localized state and parameter passing. Functions should be side-effect free.\nğŸ”¹ 09 Continuous Refactoring Good code is maintainable and extensible. Continuous refactoring reduces technical debt by identifying and fixing problems as early as possible.\nğŸ”¹ 10 Security is a Top Priority Good code should avoid common security vulnerabilities.\n"},{"id":73,"href":"/docs/network/8-popular-network-protocols/","title":"8ç§ä¸»è¦çš„ç½‘ç»œåè®®","section":"ç½‘ç»œ","content":"\n"},{"id":74,"href":"/docs/security/jwt-101-key-to-stateless-authentication.gif/","title":"JWTæ— çŠ¶æ€èº«ä»½éªŒè¯çš„å…³é”®","section":"å®‰å…¨","content":"JWT 101: Key to Stateless Authentication\nJWT or JSON Web Tokens is an open standard for securely transmitting information between two parties. They are widely used for authentication and authorization.\nA JWT consists of three main components:\n1 - Header Every JWT carries a header specifying the algorithms for signing the JWT. Itâ€™s written in JSON format.\n2 - Payload The payload consists of the claims and the user data. There are different types of claims such as registered, public, and private claims.\n3 - Signature The signature is what makes the JWT secure. It is created by taking the encoded header, encoded payload, secret key, and the algorithm and signing it.\nJWTs can be signed in two different ways:\n1 - Symmetric Signatures It uses a single secret key for both signing the token and verifying it. The same key must be shared between the server that signs the JWT and the system that verifies it.\n2 - Asymmetric Signatures In this case, a private key is used to sign the token, and a public key to verify it. The private key is kept secure on the server, while the public key can be distributed to anyone who needs to verify the token.\n"},{"id":75,"href":"/docs/db/theory/sql-query-logical-order/","title":"SQLæŸ¥è¯¢æ‰§è¡Œé¡ºåº","section":"ç†è®º","content":"\nSQL statements are executed by the database system in several steps, including: - Parsing the SQL statement and checking its validity - Transforming the SQL into an internal representation, such as relational algebra - Optimizing the internal representation and creating an execution plan that utilizes index information - Executing the plan and returning the results\n"},{"id":76,"href":"/docs/code/block/convert-video-to-gif/","title":"åˆ©ç”¨Pythonå°†è§†é¢‘è½¬åŒ–ä¸ºgif","section":"ä»£ç å—","content":" 1from moviepy import VideoFileClip 2 3 4def video_to_gif(video_path, output_path, start_time, end_time, fps=10, resize_ratio=1.0): 5 \u0026#34;\u0026#34;\u0026#34; 6 å°†è§†é¢‘ç‰‡æ®µè½¬æ¢ä¸º GIF 7 å‚æ•°ï¼š 8 video_path: è¾“å…¥è§†é¢‘æ–‡ä»¶è·¯å¾„ 9 output_path: è¾“å‡º GIF è·¯å¾„ 10 start_time: å¼€å§‹æ—¶é—´ï¼ˆç§’ï¼‰ 11 end_time: ç»“æŸæ—¶é—´ï¼ˆç§’ï¼‰ 12 fps: è¾“å‡ºå¸§ç‡ï¼ˆé»˜è®¤10ï¼‰ 13 resize_ratio: ç¼©æ”¾æ¯”ä¾‹ï¼ˆé»˜è®¤1.0ï¼‰ 14 \u0026#34;\u0026#34;\u0026#34; 15 try: 16 # åŠ è½½è§†é¢‘å¹¶æˆªå–æŒ‡å®šç‰‡æ®µ 17 with VideoFileClip(video_path) as clip: 18 subclip = clip.subclipped(start_time, end_time) 19 20 # è°ƒæ•´å°ºå¯¸ï¼ˆå¯é€‰ï¼‰ 21 if resize_ratio != 1.0: 22 subclip = subclip.resized(resize_ratio) 23 # 24 # # å¯¼å‡º GIF 25 subclip.write_gif(output_path, fps=fps) 26 27 print(f\u0026#34;GIF å·²æˆåŠŸä¿å­˜è‡³ï¼š{output_path}\u0026#34;) 28 29 except Exception as e: 30 print(f\u0026#34;å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼š{str(e)}\u0026#34;) 31 32 33# ä½¿ç”¨ç¤ºä¾‹ 34if __name__ == \u0026#34;__main__\u0026#34;: 35 video_to_gif( 36 video_path=r\u0026#39;E:\\youtube\\test.mp4\u0026#39;, 37 output_path=r\u0026#39;E:\\youtube\\test.gif\u0026#39;, 38 start_time=200, 39 end_time=210, 40 fps=15, 41 resize_ratio=0.5 42 ) "},{"id":77,"href":"/docs/network/what-makes-nginx-so-useful/","title":"Nginxå¼ºå¤§çš„åŸå› ","section":"ç½‘ç»œ","content":"\nWhat Makes Nginx So Useful\nCore Architecture: Master-worker process model handles connections efficiently High-Performance: Server Handles thousands of concurrent connections effortlessly Reverse Proxy: Distributes traffic across multiple backend servers Features: Built-in caching and SSL handling for speed and security "},{"id":78,"href":"/docs/db/theory/a-cheatsheet-on-database-performance/","title":"æ•°æ®åº“æ€§èƒ½è¯´æ˜","section":"ç†è®º","content":"Good database performance is critical since it directly impacts user experience, operational costs, and scalability.\nBut what impacts database performance?\n- Evaluating database performance depends on key metrics such as query execution time, throughput, latency, and resource utilization. - Workload Types such as write-heavy, read-heavy, delete-heavy, and competing workloads pose unique challenges. - Other factors that impact performance are item size, item type, dataset size, concurrency expectations, consistency requirements, HA expectations, and geographic distribution.\nMultiple strategies exist to improve database performance. Some of the most important ones are as follows:\n1 - Database Indexing Indexes are important for speeding up database queries by reducing the amount of data scanned. Also, choosing the right index type is crucial.\n2 - Sharding and Partitioning Divide the data into smaller, more manageable chunks known as shards. Each shard is also stored on a different server.\n3 - Denormalization Denormalization combines data into fewer tables to reduce the overhead of joins, improving read performance.\n4 - Database Replication Replication involves maintaining multiple copies of the same database, typically with a primary node for writes (and critical reads) and secondary nodes for most read operations.\n5 - Database Locking Techniques Use locking techniques like pessimistic and optimistic locking to manage concurrency levels and resource contention.\n"},{"id":79,"href":"/docs/code/theory/15-system-design-core-concepts/","title":"ç³»ç»Ÿè®¾è®¡çš„15ä¸ªæ ¸å¿ƒæ¦‚å¿µ","section":"ç†è®º","content":"\n"},{"id":80,"href":"/docs/network/how-ssh-works/","title":"SSHæ˜¯å¦‚ä½•å·¥ä½œçš„","section":"ç½‘ç»œ","content":"\nSSH Under the Hood\nSecure Shell (SSH) creates an encrypted channel between client and server. The process begins with a TCP connection, followed by version negotiation. Both parties then agree on encryption algorithms, key exchange methods, and message authentication codes. The client and server perform a key exchange (typically using Diffie-Hellman) to securely generate a shared session key for encrypting the connection. For authentication, SSH commonly uses public key authentication. The server verifies the client\u0026rsquo;s identity through a challenge-response mechanism using the client\u0026rsquo;s public key, without the private key ever being transmitted. Once authenticated, the session key encrypts all further communication, providing a secure channel. "},{"id":81,"href":"/docs/code/theory/encoding-vs-encryption-vs-tokenization/","title":"ç¼–ç /åŠ å¯†/åˆ†è¯","section":"ç†è®º","content":"\nEncoding vs Encryption vs Tokenization. Encoding, encryption, and tokenization are three distinct processes that handle data in different ways for various purposes, including data transmission, security, and compliance. In system designs, we need to select the right approach for handling sensitive information.\nğŸ”¹ Encoding Encoding converts data into a different format using a scheme that can be easily reversed. Examples include Base64 encoding, which encodes binary data into ASCII characters, making it easier to transmit data over media that are designed to deal with textual data.\nEncoding is not meant for securing data. The encoded data can be easily decoded using the same scheme without the need for a key.\nğŸ”¹ Encryption Encryption involves complex algorithms that use keys for transforming data. Encryption can be symmetric (using the same key for encryption and decryption) or asymmetric (using a public key for encryption and a private key for decryption).\nEncryption is designed to protect data confidentiality by transforming readable data (plaintext) into an unreadable format (ciphertext) using an algorithm and a secret key. Only those with the correct key can decrypt and access the original data.\nğŸ”¹ Tokenization Tokenization is the process of substituting sensitive data with non-sensitive placeholders called tokens. The mapping between the original data and the token is stored securely in a token vault. These tokens can be used in various systems and processes without exposing the original data, reducing the risk of data breaches. Tokenization is often used for protecting credit card information, personal identification numbers, and other sensitive data. Tokenization is highly secure, as the tokens do not contain any part of the original data and thus cannot be reverse-engineered to reveal the original data. It is particularly useful for compliance with regulations like PCI DSS.\n"},{"id":82,"href":"/docs/code/theory/how-we-retry-on-failures/","title":"å¦‚ä½•è¿›è¡Œå¤±è´¥é‡è¯•","section":"ç†è®º","content":"\nHow do we retry on failures?\nIn distributed systems and networked applications, retry strategies are crucial for handling transient errors and network instability effectively. The diagram shows 4 common retry strategies.\nğŸ”¹ Linear Backoff Linear backoff involves waiting for a progressively increasing fixed interval between retry attempts.\nAdvantages: Simple to implement and understand.\nDisadvantages: May not be ideal under high load or in high-concurrency environments as it could lead to resource contention or \u0026ldquo;retry storms\u0026rdquo;.\nğŸ”¹ Linear Jitter Backoff Linear jitter backoff modifies the linear backoff strategy by introducing randomness to the retry intervals. This strategy still increases the delay linearly but adds a random \u0026ldquo;jitter\u0026rdquo; to each interval.\nAdvantages: The randomness helps spread out the retry attempts over time, reducing the chance of synchronized retries across instances.\nDisadvantages: Although better than simple linear backoff, this strategy might still lead to potential issues with synchronized retries as the base interval increases only linearly.\nğŸ”¹ Exponential Backoff Exponential backoff involves increasing the delay between retries exponentially. The interval might start at 1 second, then increase to 2 seconds, 4 seconds, 8 seconds, and so on, typically up to a maximum delay. This approach is more aggressive in spacing out retries than linear backoff.\nAdvantages: Significantly reduces the load on the system and the likelihood of collision or overlap in retry attempts, making it suitable for high-load environments.\nDisadvantages: In situations where a quick retry might resolve the issue, this approach can unnecessarily delay the resolution.\nğŸ”¹ Exponential Jitter Backoff Exponential jitter backoff combines exponential backoff with randomness. After each retry, the backoff interval is exponentially increased, and then a random jitter is applied. The jitter can be either additive (adding a random amount to the exponential delay) or multiplicative (multiplying the exponential delay by a random factor).\nAdvantages: Offers all the benefits of exponential backoff, with the added advantage of reducing retry collisions even further due to the introduction of jitter.\nDisadvantages: The randomness can sometimes result in longer than necessary delays, especially if the jitter is significant.\n"},{"id":83,"href":"/docs/code/theory/uml-class-diagrams/","title":"UMLç±»å›¾è¯´æ˜","section":"ç†è®º","content":"\nUML is a standard way to visualize the design of your system and class diagrams are used across the industry.\nThey consist of:\n1 - Class Acts as the blueprint that defines the properties and behavior of an object.\n2 - Attributes Attributes in a UML class diagram represent the data fields of the class.\n3 - Methods Methods in a UML class diagram represent the behavior that a class can perform.\n4 - Interfaces Defines a contract for classes that implement it. Includes a set of methods that the implementing classes must provide.\n5 - Enumeration A special data type that defines a set of named values such as product category or months in a year.\n6 - Relationships Determines how one class is related to another. Some common relationships are as follows: - Association - Aggregation - Composition - Inheritance - Implementation\nOver to you: What other building blocks have you seen in UML class diagrams?\n"},{"id":84,"href":"/docs/code/theory/garbage-collection-101/","title":"åƒåœ¾å›æ”¶å·¥ä½œæµç¨‹","section":"ç†è®º","content":"\nHow does Garbage Collection work? Garbage collection is an automatic memory management feature used in programming languages to reclaim memory no longer used by the program.\nğŸ”¹ Java Java provides several garbage collectors, each suited for different use cases:\n\\1. Serial Garbage Collector: Best for single-threaded environments or small applications.\n\\2. Parallel Garbage Collector: Also known as the \u0026ldquo;Throughput Collector.\u0026rdquo;\n\\3. CMS (Concurrent Mark-Sweep) Garbage Collector: Low-latency collector aiming to minimize pause times.\n\\4. G1 (Garbage-First) Garbage Collector: Aims to balance throughput and latency.\n\\5. Z Garbage Collector (ZGC): A low-latency garbage collector designed for applications that require large heap sizes and minimal pause times.\nğŸ”¹ Python Python\u0026rsquo;s garbage collection is based on reference counting and a cyclic garbage collector:\n\\1. Reference Counting: Each object has a reference count; when it reaches zero, the memory is freed.\n\\2. Cyclic Garbage Collector: Handles circular references that can\u0026rsquo;t be resolved by reference counting.\nğŸ”¹ GoLang Concurrent Mark-and-Sweep Garbage Collector: Go\u0026rsquo;s garbage collector operates concurrently with the application, minimizing stop-the-world pauses.\n"}]